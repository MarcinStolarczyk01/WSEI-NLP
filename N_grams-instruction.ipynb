{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcfZ04Rafx7p"
      },
      "source": [
        "# Analiza N-gramów\n",
        "\n",
        "Celem tych laboratoriów jest zgłębienie koncepcji n-gramów poprzez implementację własnych rozwiązań i eksperymentowanie z danymi, a następnie wykorzystanie biblioteki NLTK do porównania rezultatów.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVueVw3sbifF"
      },
      "source": [
        "## Część I: Implementacja Ręczna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEBIJaQFdTBn"
      },
      "source": [
        "\n",
        "### Opis\n",
        "- **N-gramy** to sekwencje kolejnych elementów (np. słów, liter) wyodrębnianych z tekstu.\n",
        "- Przykładowo, dla zdania `\"kot i pies\"` oraz n=2 (bigramy) spodziewamy się uzyskać:  \n",
        "  `[\"kot i\", \"i pies\"]`.\n",
        "\n",
        "# Model n-gramów w Pythonie\n",
        "\n",
        "Poniższy kod umożliwia:\n",
        "- Wczytanie tekstu z pliku lub użycie domyślnego tekstu.\n",
        "- Budowanie modelu n-gramów dla zadanej wartości n (dopuszczalne: 2, 3, 4, 8, 16).\n",
        "- Wyświetlenie przykładowych n-gramów z listą słów, które następują po nich.\n",
        "- Generowanie tekstu na podstawie wytrenowanego modelu.\n",
        "\n",
        "## Kod źródłowy\n",
        "\n",
        "```python\n",
        "import random\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "def build_ngram_model(text, n):\n",
        "    \"\"\"\n",
        "    Buduje model n-gramów na podstawie zadanego tekstu.\n",
        "    \n",
        "    :param text: Ciąg tekstowy wejściowy.\n",
        "    :param n: Liczba tokenów tworzących n-gram.\n",
        "    :return: Słownik, którego klucze to krotki zawierające n kolejnych tokenów,\n",
        "             a wartości to lista słów występujących po danym n-gramie.\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "    model = {}\n",
        "    for i in range(len(tokens) - n):\n",
        "        gram = tuple(tokens[i:i+n])\n",
        "        next_word = tokens[i+n]\n",
        "        if gram not in model:\n",
        "            model[gram] = []\n",
        "        model[gram].append(next_word)\n",
        "    return model\n",
        "\n",
        "def generate_text(model, n, length, seed=None):\n",
        "    \"\"\"\n",
        "    Generuje tekst na podstawie modelu n-gramów.\n",
        "    \n",
        "    :param model: Model n-gramów w postaci słownika.\n",
        "    :param n: Liczba tokenów w n-gramie.\n",
        "    :param length: Liczba dodatkowych słów do wygenerowania.\n",
        "    :param seed: Opcjonalny początkowy n-gram (krotka). Jeżeli nie podany, wybierany jest losowo.\n",
        "    :return: Wygenerowany ciąg tekstowy.\n",
        "    \"\"\"\n",
        "    if seed is None:\n",
        "        seed = random.choice(list(model.keys()))\n",
        "    output = list(seed)\n",
        "    current = seed\n",
        "    for _ in range(length):\n",
        "        possibilities = model.get(current)\n",
        "        if not possibilities:\n",
        "            break  # brak kontynuacji - kończymy generowanie\n",
        "        next_word = random.choice(possibilities)\n",
        "        output.append(next_word)\n",
        "        current = tuple(output[-n:])\n",
        "    return ' '.join(output)\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Generowanie tekstu na podstawie modelu n-gramów. \"\n",
        "                    \"Wybierz wartość n oraz opcjonalnie podaj plik z tekstem.\"\n",
        "    )\n",
        "    parser.add_argument('--n', type=int, default=2,\n",
        "                        help=\"Wartość n dla n-gramów (dopuszczalne: 2, 3, 4, 8, 16)\")\n",
        "    parser.add_argument('--length', type=int, default=20,\n",
        "                        help=\"Liczba dodatkowych słów do wygenerowania (domyślnie 20)\")\n",
        "    parser.add_argument('--file', type=str,\n",
        "                        help=\"Ścieżka do pliku tekstowego (jeżeli nie podano, używany jest przykładowy tekst)\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Wczytywanie tekstu z pliku lub użycie tekstu przykładowego\n",
        "    if args.file:\n",
        "        try:\n",
        "            with open(args.file, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"Błąd przy otwieraniu pliku: {e}\")\n",
        "            sys.exit(1)\n",
        "    else:\n",
        "        text = (\n",
        "            \"To jest przykładowy tekst do demonstracji implementacji modelu n-gramów. \"\n",
        "            \"Model n-gramów pozwala analizować sekwencje słów i generować nowe teksty na podstawie wzorców. \"\n",
        "            \"Wybierz odpowiednią wartość n, aby zobaczyć, jak zmienia się generowany tekst. \"\n",
        "            \"Im większa wartość n, tym model bierze pod uwagę dłuższy kontekst, co może wpływać na spójność i precyzję generowanego tekstu. \"\n",
        "            \"Eksperymentuj z różnymi ustawieniami, np. 2, 3, 4, 8, 16-gramów.\"\n",
        "        )\n",
        "    # Usunięcie zbędnych znaków nowej linii (jeśli występują)\n",
        "    text = text.replace('\\n', ' ')\n",
        "\n",
        "    # Sprawdzamy, czy wybrana wartość n jest dopuszczalna\n",
        "    if args.n not in [2, 3, 4, 8, 16]:\n",
        "        print(\"Proszę wybrać wartość n spośród: 2, 3, 4, 8, 16.\")\n",
        "        sys.exit(1)\n",
        "    \n",
        "    n = args.n\n",
        "    print(f\"\\nBudowanie modelu {n}-gramów...\")\n",
        "    model = build_ngram_model(text, n)\n",
        "\n",
        "    # Wyświetlenie kilku przykładowych n-gramów z listą możliwych kontynuacji\n",
        "    print(\"\\nPrzykładowe n-gramy i słowa, które następują po nich:\")\n",
        "    for idx, (gram, next_words) in enumerate(model.items()):\n",
        "        print(f\"{gram} -> {next_words}\")\n",
        "        if idx >= 4:\n",
        "            break\n",
        "\n",
        "    # Generowanie tekstu na podstawie modelu n-gramów\n",
        "    print(\"\\nGenerowanie tekstu:\")\n",
        "    generated_text = generate_text(model, n, args.length)\n",
        "    print(generated_text)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZgRGTuJjgFh"
      },
      "source": [
        "# Implementacja Laplace Smoothing dla modelu bigramów\n",
        "\n",
        "Laplace smoothing (wygładzanie add-one) to metoda pozwalająca na uniknięcie zerowych prawdopodobieństw w modelach językowych, szczególnie przy modelach n-gramowych. Polega ona na dodaniu 1 do liczby wystąpień każdego n-gramu, dzięki czemu nawet nieobserwowane sekwencje otrzymują niezerową wartość prawdopodobieństwa.\n",
        "\n",
        "## Matematyka\n",
        "Dla danego kontekstu \\(h\\) oraz kolejnego słowa \\(w\\), prawdopodobieństwo oblicza się według wzoru:\n",
        "\n",
        "$$\n",
        "P(w \\mid h) = \\frac{\\mathrm{count}(h, w) + 1}{\\mathrm{count}(h) + V}\n",
        "$$\n",
        "\n",
        "gdzie:\n",
        "- count(h, w) – liczba wystąpień sekwencji \\(h, w\\),\n",
        "- count(h) – liczba wystąpień kontekstu \\(h\\),\n",
        "- (V) – rozmiar słownika (liczba unikalnych słów).\n",
        "\n",
        "\n",
        "## Przykładowa implementacja w Pythonie\n",
        "\n",
        "```python\n",
        "import random\n",
        "\n",
        "def build_bigram_counts(text):\n",
        "    \"\"\"\n",
        "    Buduje liczniki bigramów na podstawie tekstu.\n",
        "    \n",
        "    :param text: Ciąg tekstowy.\n",
        "    :return: Słownik, w którym kluczem jest słowo, a wartością inny słownik z licznikami słów występujących po danym słowie.\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "    counts = {}\n",
        "    for i in range(len(tokens) - 1):\n",
        "        word = tokens[i]\n",
        "        next_word = tokens[i + 1]\n",
        "        if word not in counts:\n",
        "            counts[word] = {}\n",
        "        counts[word][next_word] = counts[word].get(next_word, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def laplace_probability(counts, word, next_word, vocab_size):\n",
        "    \"\"\"\n",
        "    Oblicza prawdopodobieństwo wystąpienia 'next_word' po 'word' z wykorzystaniem Laplace smoothing.\n",
        "    \n",
        "    :param counts: Słownik liczników bigramów.\n",
        "    :param word: Słowo-historyczne.\n",
        "    :param next_word: Słowo, dla którego liczymy prawdopodobieństwo.\n",
        "    :param vocab_size: Rozmiar słownika (liczba unikalnych słów w korpusie).\n",
        "    :return: Wygładzona wartość prawdopodobieństwa.\n",
        "    \"\"\"\n",
        "    count_bigram = counts.get(word, {}).get(next_word, 0)\n",
        "    total_count = sum(counts.get(word, {}).values())\n",
        "    return (count_bigram + 1) / (total_count + vocab_size)\n",
        "\n",
        "# Przykładowy tekst\n",
        "text = (\n",
        "    \"To jest przykładowy tekst do demonstracji modelu bigramów. \"\n",
        "    \"Model bigramów jest prostym modelem językowym.\"\n",
        ")\n",
        "\n",
        "# Budujemy liczniki bigramów\n",
        "bigram_counts = build_bigram_counts(text)\n",
        "print(\"Bigram counts:\")\n",
        "for word, next_words in bigram_counts.items():\n",
        "    print(f\"{word}: {next_words}\")\n",
        "\n",
        "# Przygotowanie słownika - zestaw unikalnych słów\n",
        "vocab = set(text.split())\n",
        "vocab_size = len(vocab)\n",
        "print(f\"\\nRozmiar słownika: {vocab_size}\")\n",
        "\n",
        "# Obliczenie prawdopodobieństwa dla konkretnego bigramu z Laplace smoothing\n",
        "word = \"bigramów\"\n",
        "next_word = \"jest\"\n",
        "prob = laplace_probability(bigram_counts, word, next_word, vocab_size)\n",
        "print(f\"\\nPrawdopodobieństwo wystąpienia słowa '{next_word}' po '{word}': {prob:.4f}\")\n",
        "\n",
        "# Przykładowe obliczenie dla nieobserwowanego bigramu\n",
        "unknown_next = \"nieistnieje\"\n",
        "prob_unknown = laplace_probability(bigram_counts, word, unknown_next, vocab_size)\n",
        "print(f\"Prawdopodobieństwo wystąpienia słowa '{unknown_next}' po '{word}': {prob_unknown:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3rnlNglb8pn"
      },
      "source": [
        "## Część II: Wykorzystanie NLTK w N-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpGSUrT2gH2G"
      },
      "source": [
        "\n",
        "1. **Tokenizacja i Generacja n-gramów:**\n",
        "   - Użyj funkcji `nltk.word_tokenize` do podziału tekstu na słowa.\n",
        "   - Wykorzystaj funkcje takie jak `nltk.ngrams`, `nltk.bigrams` lub `nltk.trigrams` do wygenerowania n-gramów.\n",
        "\n",
        "2. **Budowa Modelu Językowego:**\n",
        "   - Wybierz korpus dostępny w NLTK (np. `reuters` lub `brown`).\n",
        "   - Tokenizuj teksty z wybranego korpusu.\n",
        "   - Wygeneruj trigrams (lub n-gramy o wybranej długości).\n",
        "   - Zbuduj model, który zapisze częstotliwość występowania kolejnych słów po danej sekwencji (np. dla pary słów).\n",
        "   - Znormalizuj liczniki do postaci prawdopodobieństw.\n",
        "\n",
        "### Przykładowy kod:\n",
        "```python\n",
        "import nltk\n",
        "from nltk import trigrams\n",
        "from nltk.corpus import reuters\n",
        "from collections import defaultdict\n",
        "\n",
        "# Pobranie niezbędnych zasobów\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenizacja tekstu z korpusu Reuters\n",
        "words = nltk.word_tokenize(' '.join(reuters.words()))\n",
        "\n",
        "# Generowanie trigrams\n",
        "tri_grams = list(trigrams(words))\n",
        "\n",
        "# Budowa modelu trigramowego\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "for w1, w2, w3 in tri_grams:\n",
        "    model[(w1, w2)][w3] += 1\n",
        "\n",
        "# Normalizacja do prawdopodobieństw\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count\n",
        "\n",
        "def predict_next_word(w1, w2):\n",
        "    \"\"\"\n",
        "    Przewiduje kolejne słowo na podstawie dwóch poprzednich słów przy użyciu modelu trigramowego.\n",
        "    \"\"\"\n",
        "    next_words = model[(w1, w2)]\n",
        "    if next_words:\n",
        "        return max(next_words, key=next_words.get)\n",
        "    else:\n",
        "        return \"Brak predykcji\"\n",
        "\n",
        "# Przykład użycia funkcji predykcji\n",
        "print(\"Next Word:\", predict_next_word('the', 'stock'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsJ1bBZ7dB5w"
      },
      "source": [
        "## Część III: Metryki Modelowania Języka"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ5noMZygMk3"
      },
      "source": [
        "\n",
        "Aby ocenić jakość modelu językowego, wykorzystujemy kilka kluczowych metryk:\n",
        "\n",
        "### 1. Entropia\n",
        "\n",
        "Entropia mierzy ilość informacji zawartej w rozkładzie prawdopodobieństwa. Wzór:\n",
        "  \n",
        "$$\n",
        "H(p) = \\sum_{x} p(x) \\cdot \\left(-\\log(p(x))\\right)\n",
        "$$\n",
        "\n",
        "> **Ważne:** Wartość entropii \\( H(p) \\) zawsze jest większa lub równa 0.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Cross-Entropy\n",
        "\n",
        "Cross-Entropy (krzyżowa entropia) ocenia, jak dobrze model przewiduje dane testowe. Dla ciągu słów \\( w_1, w_2, \\dots, w_N \\) wzór jest następujący:\n",
        "\n",
        "$$\n",
        "H(p) = \\sum_{i=1}^{N} \\left(-\\log_2 \\left(p(w_i \\mid w_1^{i-1})\\right)\\right)\n",
        "$$\n",
        "\n",
        "> **Uwaga:** Cross-Entropy jest zawsze większa lub równa entropii – model nie może być bardziej pewny niż prawdziwa niepewność danych.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Perplexity\n",
        "\n",
        "Perplexity jest miarą niepewności modelu i jakości jego przewidywań. Można ją obliczyć z wykorzystaniem cross-entropy:\n",
        "\n",
        "$$\n",
        "\\text{Perplexity} = 2^{H(p)}\n",
        "$$\n",
        "\n",
        "Alternatywnie, dla zbioru testowego, wzór przyjmuje postać:\n",
        "\n",
        "$$\n",
        "\\text{PP}(W) = \\left( \\prod_{i=1}^{N} P(w_i \\mid w_{i-1}) \\right)^{-\\frac{1}{N}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Przykład Obliczeń\n",
        "\n",
        "Rozważmy zdanie: **\"Natural Language Processing\"**.\n",
        "\n",
        "#### Krok 1: Prawdopodobieństwa\n",
        "\n",
        "Dla pierwszego słowa (przyjmując `<start>` jako kontekst):\n",
        "\n",
        "| Słowo      | \\( P(\\text{word} \\mid \\text{<start>}) \\) |\n",
        "|------------|-----------------------------------------|\n",
        "| The        | 0.4                                     |\n",
        "| Processing | 0.3                                     |\n",
        "| Natural    | 0.12                                    |\n",
        "| Language   | 0.18                                    |\n",
        "\n",
        "Zakładamy, że pierwszym wybranym słowem jest **\"Natural\"**.\n",
        "\n",
        "Dla słowa po **\"Natural\"**:\n",
        "\n",
        "| Słowo      | \\( P(\\text{word} \\mid \\text{Natural}) \\) |\n",
        "|------------|------------------------------------------|\n",
        "| The        | 0.05                                     |\n",
        "| Processing | 0.3                                      |\n",
        "| Natural    | 0.15                                     |\n",
        "| Language   | 0.5                                      |\n",
        "\n",
        "Wybieramy **\"Language\"**.\n",
        "\n",
        "Dla słowa po **\"Language\"**:\n",
        "\n",
        "| Słowo      | \\( P(\\text{word} \\mid \\text{Language}) \\) |\n",
        "|------------|------------------------------------------|\n",
        "| The        | 0.1                                      |\n",
        "| Processing | 0.7                                      |\n",
        "| Natural    | 0.1                                      |\n",
        "| Language   | 0.1                                      |\n",
        "\n",
        "Wybieramy **\"Processing\"**.\n",
        "\n",
        "#### Krok 2: Obliczenie Perplexity\n",
        "\n",
        "Obliczamy iloczyn prawdopodobieństw:\n",
        "\n",
        "$$\n",
        "0.12 \\times 0.5 \\times 0.7 = 0.042\n",
        "$$\n",
        "\n",
        "Perplexity wyliczamy jako:\n",
        "\n",
        "$$\n",
        "\\text{PP}(W) = \\left(0.042\\right)^{-\\frac{1}{3}} \\approx 2.876\n",
        "$$\n",
        "\n",
        "#### Krok 3: Obliczenie Entropii\n",
        "\n",
        "Entropię uzyskujemy jako logarytm dwójkowy z wartości perplexity:\n",
        "\n",
        "$$\n",
        "\\text{Entropy} = \\log_2(2.876) \\approx 1.524\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Podsumowanie\n",
        "\n",
        "- **Entropia:** Mierzy ilość informacji zawartej w rozkładzie prawdopodobieństwa.\n",
        "- **Cross-Entropy:** Ocena, jak dobrze model przewiduje dane testowe – zawsze większa lub równa entropii.\n",
        "- **Perplexity:** Miara niepewności modelu; im niższa wartość, tym lepsze przewidywania.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJp4VMQGhM3K"
      },
      "source": [
        "Dodatkowo laboratoria mają na celu zapoznanie z metodami Bag of Words, TF-IDF i embeddingów."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMc43c3zdwW-"
      },
      "source": [
        "# Metody reprezentacji tekstu\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyPMNsaEgQK9"
      },
      "source": [
        "Bag-of-Words (BoW) to jedna z najprostszych metod reprezentacji tekstu, która polega na zamianie dokumentu na wektor liczbowy. W tej metodzie tekst traktowany jest jako \"worek\" słów, gdzie istotna jest jedynie liczba wystąpień poszczególnych tokenów, a kolejność słów zostaje zignorowana.\n",
        "\n",
        "**Proces przetwarzania:**\n",
        "- **Tokenizacja:** Podział tekstu na mniejsze jednostki (słowa, tokeny).\n",
        "- **Czyszczenie:** Usuwanie znaków interpunkcyjnych, liczb oraz konwersja tekstu do formy jednolitej (np. na małe litery).\n",
        "- **Usuwanie stop-słów:** Eliminacja często występujących, ale mało informacyjnych słów (np. „i”, „oraz”, „ale”).\n",
        "- **Budowanie słownika:** Utworzenie listy unikalnych słów, które występują w korpusie.\n",
        "- **Tworzenie wektora:** Reprezentacja dokumentu jako wektor częstotliwości występowania poszczególnych słów.\n",
        "\n",
        "**Biblioteki:**\n",
        "- **scikit-learn:** Narzędzie `CountVectorizer` umożliwia szybkie przekształcenie tekstu na macierz cech.\n",
        "- **NLTK:** Umożliwia tokenizację, usuwanie stop-słów, stemizację i lematyzację.\n",
        "- **spaCy:** Zaawansowane narzędzie NLP, które również pozwala na tokenizację oraz inne operacje przetwarzania języka.\n",
        "\n",
        "**Zalety i wady:**\n",
        "- **Zalety:** Prosta implementacja, efektywna przy analizie częstotliwości słów.\n",
        "- **Wady:** Utrata informacji o kolejności słów i kontekście, co może ograniczać zdolność uchwycenia semantyki dokumentu.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqqnqO3qljBC"
      },
      "source": [
        "# Przykład 1: Bag-of-Words (BoW) przy użyciu scikit-learn\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Przykładowy korpus dokumentów\n",
        "documents = [\n",
        "    \"To jest przykładowy tekst.\",\n",
        "    \"Drugi dokument z innym tekstem.\",\n",
        "    \"Trzeci dokument zawiera przykładowy tekst.\"\n",
        "]\n",
        "\n",
        "# Inicjalizacja CountVectorizer z polskimi stop-słowami\n",
        "vectorizer = CountVectorizer(stop_words='polish')\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Wyświetlenie słownika (wszystkich tokenów)\n",
        "print(\"Słownik tokenów:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Wyświetlenie macierzy Bag-of-Words\n",
        "print(\"\\nMacierz Bag-of-Words:\")\n",
        "print(bow_matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhByUtmfghAN"
      },
      "source": [
        "# TF-IDF (Term Frequency – Inverse Document Frequency)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQq5A0hrgkAw"
      },
      "source": [
        "\n",
        "TF-IDF to technika oceny ważności słowa w dokumencie, która bierze pod uwagę zarówno jego częstotliwość występowania w danym dokumencie (TF), jak i rzadkość występowania w całym zbiorze dokumentów (IDF). Dzięki temu słowa charakterystyczne dla danego dokumentu są wyróżniane, a te powszechnie występujące zyskują mniejszą wagę.\n",
        "\n",
        "**Proces przetwarzania:**\n",
        "- **Obliczanie TF (Term Frequency):** Mierzy, jak często dane słowo pojawia się w danym dokumencie.\n",
        "- **Obliczanie IDF (Inverse Document Frequency):** Mierzy, jak unikalne jest dane słowo w całym zbiorze dokumentów, zazwyczaj przy użyciu logarytmu odwrotnej proporcji liczby dokumentów zawierających dane słowo.\n",
        "- **Kombinacja:** Mnożenie TF przez IDF daje wynikową wagę, która wskazuje na znaczenie słowa w kontekście dokumentu i korpusu.\n",
        "\n",
        "**Biblioteki:**\n",
        "- **scikit-learn:** `TfidfVectorizer` umożliwia automatyczne obliczanie macierzy TF-IDF na podstawie zbioru dokumentów.\n",
        "- **NLTK:** Może być wykorzystywany do wstępnej obróbki tekstu (tokenizacja, usuwanie stop-słów) przed obliczeniem TF-IDF.\n",
        "- **spaCy:** Ułatwia przygotowanie danych poprzez zaawansowaną analizę składniową i tokenizację.\n",
        "\n",
        "**Zalety i wady:**\n",
        "- **Zalety:** Lepsze od BoW przy wyłapywaniu istotnych słów, ponieważ uwzględnia unikalność słów w korpusie.\n",
        "- **Wady:** W przypadku bardzo dużych zbiorów danych obliczenia mogą być kosztowne, a metoda nie przechwytuje semantycznych zależności między słowami.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_lSb6ozllO_"
      },
      "source": [
        "\n",
        "```markdown\n",
        "# Przykład 2: TF-IDF przy użyciu scikit-learn\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Przykładowy korpus dokumentów\n",
        "documents = [\n",
        "    \"To jest przykładowy tekst.\",\n",
        "    \"Drugi dokument z innym tekstem.\",\n",
        "    \"Trzeci dokument zawiera przykładowy tekst.\"\n",
        "]\n",
        "\n",
        "# Inicjalizacja TfidfVectorizer z polskimi stop-słowami\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='polish')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Wyświetlenie słownika (wszystkich tokenów)\n",
        "print(\"Słownik tokenów (TF-IDF):\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Wyświetlenie macierzy TF-IDF\n",
        "print(\"\\nMacierz TF-IDF:\")\n",
        "print(tfidf_matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTMe48Jhgmr7"
      },
      "source": [
        "# Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XatMPtmsgoQY"
      },
      "source": [
        "Word Embeddings to metoda reprezentacji słów, która mapuje każde słowo na wektor w przestrzeni wielowymiarowej. Dzięki temu słowa o podobnym znaczeniu mają wektory umieszczone blisko siebie, co pozwala na uchwycenie zależności semantycznych i syntaktycznych między nimi. Modele te uczą się reprezentacji na podstawie kontekstu, w jakim słowa występują w dużych zbiorach tekstowych.\n",
        "\n",
        "**Popularne modele:**\n",
        "- **Word2Vec:** Używa architektur CBOW (Continuous Bag of Words) lub Skip-Gram do nauki wektorowych reprezentacji słów na podstawie ich kontekstu.\n",
        "- **GloVe:** Wykorzystuje statystyki współwystępowania słów w korpusie, aby uzyskać globalną reprezentację wektorową.\n",
        "- **FastText:** Rozszerzenie Word2Vec, które uwzględnia również wewnętrzną strukturę słów, dzieląc je na mniejsze fragmenty (n-gramy).\n",
        "\n",
        "**Biblioteki:**\n",
        "- **gensim:** Bardzo popularna biblioteka do trenowania modeli Word2Vec, FastText oraz korzystania z gotowych modeli embeddingowych.\n",
        "- **TensorFlow/Keras:** Umożliwiają budowanie niestandardowych modeli sieci neuronowych, w tym warstw embeddingowych, co pozwala na głębszą integrację z zadaniami uczenia maszynowego.\n",
        "- **PyTorch:** Alternatywna platforma do trenowania modeli sieci neuronowych, oferująca elastyczność w implementacji modeli embeddingowych.\n",
        "\n",
        "**Zalety i wady:**\n",
        "- **Zalety:** Pozwalają na uchwycenie głębszych relacji semantycznych między słowami, co znacząco poprawia wydajność w zadaniach takich jak klasyfikacja tekstu, analiza sentymentu czy tłumaczenie maszynowe.\n",
        "- **Wady:** Wymagają dużych zbiorów danych do skutecznego treningu, a ich trenowanie bywa czasochłonne i wymaga znacznych zasobów obliczeniowych.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PglM96wNlnF1"
      },
      "source": [
        "\n",
        "```markdown\n",
        "# Przykład 3: Word Embeddings przy użyciu gensim (Word2Vec)\n",
        "\n",
        "```python\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Przygotowanie przykładowego korpusu - tokenizacja tekstów\n",
        "sentences = [\n",
        "    simple_preprocess(\"To jest przykładowy tekst.\"),\n",
        "    simple_preprocess(\"Drugi dokument z innym tekstem.\"),\n",
        "    simple_preprocess(\"Trzeci dokument zawiera przykładowy tekst.\")\n",
        "]\n",
        "\n",
        "# Trenowanie modelu Word2Vec\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Wybór słowa do analizy\n",
        "word = \"tekst\"\n",
        "\n",
        "# Sprawdzenie, czy słowo istnieje w modelu oraz wyświetlenie jego wektora\n",
        "if word in model.wv:\n",
        "    print(f\"Wektor dla słowa '{word}':\")\n",
        "    print(model.wv[word])\n",
        "else:\n",
        "    print(f\"Słowo '{word}' nie występuje w modelu.\")\n",
        "\n",
        "# Znalezienie 3 najbliższych (podobnych) słów\n",
        "similar_words = model.wv.most_similar(word, topn=3)\n",
        "print(f\"\\nNajbardziej podobne słowa do '{word}':\")\n",
        "for similar_word, similarity in similar_words:\n",
        "    print(f\"{similar_word}: {similarity:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TotGokBUmndh"
      },
      "source": [
        "# Zadanie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFKjXs2ompZV"
      },
      "source": [
        "# Zadania do zrobienia: Reprezentacja Tekstu i Analiza N-gramów\n",
        "\n",
        "[Dataset 20newsgroups](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Wybieramy kilka kategorii do analizy\n",
        "categories = ['sci.med', 'sci.space', 'rec.sport.baseball', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "documents = newsgroups.data\n",
        "\n",
        "print(f\"Pobrano {len(documents)} dokumentów.\")\n",
        "\n",
        "\n",
        "\n",
        "# Zadanie 1: Reprezentacja tekstu przy użyciu metody Bag-of-Words (BoW)\n",
        "\n",
        "**Polecenie:**\n",
        "- Wczytaj dokumenty z wbudowanego datasetu (np. 20 Newsgroups).\n",
        "- Wykonaj tokenizację tekstu oraz usuń stop-słowa.\n",
        "- Zbuduj reprezentację tekstu metodą Bag-of-Words, tworząc macierz wystąpień słów.\n",
        "- Wyświetl najczęściej występujące słowa wraz z ich liczebnością.\n",
        "\n",
        "---\n",
        "\n",
        "# Zadanie 2: Obliczanie TF-IDF dla zbioru dokumentów\n",
        "\n",
        "**Polecenie:**\n",
        "- Wczytaj dokumenty z wbudowanego datasetu.\n",
        "- Przetwórz tekst, wykonując tokenizację oraz usuwanie stop-słów.\n",
        "- Oblicz macierz TF-IDF dla całego zbioru dokumentów.\n",
        "- Dla wybranego dokumentu wypisz słowa o najwyższych wartościach TF-IDF.\n",
        "\n",
        "---\n",
        "\n",
        "# Zadanie 3: Implementacja Word Embeddings\n",
        "\n",
        "**Polecenie:**\n",
        "- Wczytaj i przetwórz dokumenty z wbudowanego datasetu (tokenizacja, usuwanie stop-słów, normalizacja).\n",
        "- Wytrenuj model Word Embeddings (np. Word2Vec lub GloVe) na przetworzonym korpusie.\n",
        "- Przetestuj model, wyszukując najbliższe wektory (sąsiadów) dla wybranego słowa.\n",
        "\n",
        "---\n",
        "\n",
        "# Zadanie 4: Generowanie i analiza Bigramów\n",
        "\n",
        "**Polecenie:**\n",
        "- Wczytaj dokumenty z wbudowanego datasetu.\n",
        "- Wykonaj tokenizację tekstu oraz usuń stop-słowa.\n",
        "- Wygeneruj bigramy (pary kolejnych słów) z przetworzonego tekstu.\n",
        "- Wyświetl najczęściej występujące bigramy wraz z ich liczebnością.\n",
        "\n",
        "---\n",
        "\n",
        "# Zadanie 5: Analiza Trigramów w tekście\n",
        "\n",
        "**Polecenie:**\n",
        "- Wczytaj dokumenty z wbudowanego datasetu.\n",
        "- Przetwórz tekst, wykonując tokenizację oraz usuwanie stop-słów.\n",
        "- Wygeneruj trigramy (sekwencje trzech kolejnych słów) z dokumentów.\n",
        "- Wypisz najczęściej występujące trigramy w analizowanym zbiorze.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVo2qJsrnXU7"
      },
      "source": [
        "Więcej informacji:\n",
        "\n",
        "- [Word2vec](https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
        ")\n",
        "\n",
        "- [TFIDF](https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/\n",
        ")\n",
        "\n",
        "- [N-grams NLTK](https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk/)\n",
        "\n",
        "- [NLTK](https://www.geeksforgeeks.org/tokenize-text-using-nltk-python/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
