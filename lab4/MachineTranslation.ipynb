{
   "cells": [
      {
         "cell_type": "markdown",
         "id": "110bce21",
         "metadata": {},
         "source": [
            "# Machine translation"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "18906a97",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:04:08.509991Z",
               "iopub.status.busy": "2025-07-04T16:04:08.509446Z",
               "iopub.status.idle": "2025-07-04T16:05:21.161542Z",
               "shell.execute_reply": "2025-07-04T16:05:21.160839Z",
               "shell.execute_reply.started": "2025-07-04T16:04:08.509959Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
                  "To disable this warning, you can either:\n",
                  "\t- Avoid using `tokenizers` before the fork if possible\n",
                  "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
               ]
            }
         ],
         "source": [
            "!pip install -q datasets sentencepiece transformers torch transformers[torch] dotenv\n",
            "\n",
            "import dotenv\n",
            "\n",
            "dotenv.load_dotenv()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "221d82ab",
         "metadata": {},
         "source": [
            "## Translation using transformer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 30,
         "id": "c2b2223f",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:05:21.163544Z",
               "iopub.status.busy": "2025-07-04T16:05:21.163073Z",
               "iopub.status.idle": "2025-07-04T16:05:21.167484Z",
               "shell.execute_reply": "2025-07-04T16:05:21.166764Z",
               "shell.execute_reply.started": "2025-07-04T16:05:21.163518Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "pl_text = \"Wczesnym rankiem, gdy słońce dopiero zaczynało wschodzić, ulice miasta były jeszcze puste. Czuć było świeżość powietrza i delikatny zapach kawy unoszący się z pobliskiej kawiarni. To był idealny moment na spokojny spacer przed rozpoczęciem dnia.\"\n",
            "en_text = \"Early in the morning, as the sun was just beginning to rise, the city streets were still empty. The air felt fresh, and a gentle smell of coffee drifted from a nearby café. It was the perfect moment for a peaceful walk before the day began.\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 31,
         "id": "73d660c3",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:05:21.168462Z",
               "iopub.status.busy": "2025-07-04T16:05:21.168209Z",
               "iopub.status.idle": "2025-07-04T16:05:51.282432Z",
               "shell.execute_reply": "2025-07-04T16:05:51.281666Z",
               "shell.execute_reply.started": "2025-07-04T16:05:21.168441Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-pl-en\")\n",
            "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-pl-en\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 32,
         "id": "79d45420",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:05:51.284553Z",
               "iopub.status.busy": "2025-07-04T16:05:51.283969Z",
               "iopub.status.idle": "2025-07-04T16:05:53.860240Z",
               "shell.execute_reply": "2025-07-04T16:05:53.859449Z",
               "shell.execute_reply.started": "2025-07-04T16:05:51.284525Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "tokenized_text_pl = tokenizer(pl_text, return_tensors=\"pt\")\n",
            "translated_tokens = model.generate(\n",
            "    input_ids=tokenized_text_pl[\"input_ids\"],\n",
            "    attention_mask=tokenized_text_pl[\"attention_mask\"],\n",
            ")\n",
            "translated = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 33,
         "id": "89579bff",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:05:53.861426Z",
               "iopub.status.busy": "2025-07-04T16:05:53.861142Z",
               "iopub.status.idle": "2025-07-04T16:05:53.865930Z",
               "shell.execute_reply": "2025-07-04T16:05:53.865180Z",
               "shell.execute_reply.started": "2025-07-04T16:05:53.861385Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Polish text:\n",
                  " Wczesnym rankiem, gdy słońce dopiero zaczynało wschodzić, ulice miasta były jeszcze puste. Czuć było świeżość powietrza i delikatny zapach kawy unoszący się z pobliskiej kawiarni. To był idealny moment na spokojny spacer przed rozpoczęciem dnia.\n",
                  "Translated text:\n",
                  " Early in the morning, when the sun was just beginning to rise, the streets of the city were still empty. It felt fresh air and a gentle smell of coffee floating from a nearby cafe. It was the perfect moment for a quiet walk before the beginning of the day.\n",
                  "Expected translation:\n",
                  " Early in the morning, as the sun was just beginning to rise, the city streets were still empty. The air felt fresh, and a gentle smell of coffee drifted from a nearby café. It was the perfect moment for a peaceful walk before the day began.\n"
               ]
            }
         ],
         "source": [
            "print(f\"Polish text:\\n {pl_text}\")\n",
            "print(f\"Translated text:\\n {translated}\")\n",
            "print(f\"Expected translation:\\n {en_text}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "c5f6d99b",
         "metadata": {},
         "source": [
            "## Language detection"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 34,
         "id": "82b33306",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:05:53.866702Z",
               "iopub.status.busy": "2025-07-04T16:05:53.866518Z",
               "iopub.status.idle": "2025-07-04T16:05:59.278132Z",
               "shell.execute_reply": "2025-07-04T16:05:59.277355Z",
               "shell.execute_reply.started": "2025-07-04T16:05:53.866686Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
                  "To disable this warning, you can either:\n",
                  "\t- Avoid using `tokenizers` before the fork if possible\n",
                  "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
               ]
            }
         ],
         "source": [
            "!pip install -q langdetect"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 35,
         "id": "00d84ff4",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:05:59.279546Z",
               "iopub.status.busy": "2025-07-04T16:05:59.279218Z",
               "iopub.status.idle": "2025-07-04T16:05:59.654240Z",
               "shell.execute_reply": "2025-07-04T16:05:59.653477Z",
               "shell.execute_reply.started": "2025-07-04T16:05:59.279503Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Detected language: hu\n"
               ]
            }
         ],
         "source": [
            "from langdetect import detect\n",
            "\n",
            "source_text = \"Az ezer mérföldes utazás is egyetlen lépéssel kezdődik.\"\n",
            "print(f\"Detected language: {detect(source_text)}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "2742b143",
         "metadata": {},
         "source": [
            "## Language detection and translation with HERBERT model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 36,
         "id": "f16a8844",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:05:59.655241Z",
               "iopub.status.busy": "2025-07-04T16:05:59.654945Z",
               "iopub.status.idle": "2025-07-04T16:06:08.611413Z",
               "shell.execute_reply": "2025-07-04T16:06:08.610663Z",
               "shell.execute_reply.started": "2025-07-04T16:05:59.655215Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
                  "To disable this warning, you can either:\n",
                  "\t- Avoid using `tokenizers` before the fork if possible\n",
                  "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
               ]
            }
         ],
         "source": [
            "!pip install -q googletrans==4.0.0-rc1 nltk"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 37,
         "id": "f480a542",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:06:08.612885Z",
               "iopub.status.busy": "2025-07-04T16:06:08.612577Z",
               "iopub.status.idle": "2025-07-04T16:06:19.189197Z",
               "shell.execute_reply": "2025-07-04T16:06:19.188285Z",
               "shell.execute_reply.started": "2025-07-04T16:06:08.612852Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "import googletrans\n",
            "\n",
            "text = \"\"\"Szklanka ma u mnie pojemność 250 ml.\n",
            "Do usmażenia naleśników użyłam szerokiej patelni do naleśników: średnica 24 cm.\n",
            "Z podanej ilości składników wyszło mi 16 bardzo cienkich naleśników. \n",
            "Jajka wyjmij wcześniej z lodówki. Możesz też lekko podgrzać mleko do temperatury nie wyższej niż 40 stopni.\n",
            "\n",
            "Kalorie policzone zostały na podstawie użytych przeze mnie składników. Jest to więc orientacyjna ilość kalorii, ponieważ nawet mąka może mieć inną ilość kalorii niż ta, której użyłam ja. \n",
            "\n",
            "Naleśniki szykuję przynajmniej raz w tygodniu. To danie proste, szybkie i uwielbiane przez dzieci. Możesz je podać na dowolny posiłek w ciągu dnia. Naleśniki możesz zabrać do pracy, do szkoły lub na piknik. Są po prostu boskie!\"\"\".replace(\n",
            "    \"\\n\", \" \"\n",
            ")\n",
            "translator = googletrans.Translator()\n",
            "\n",
            "\n",
            "def translate(text, dest) -> str:\n",
            "    return translator.translate(text, dest=dest, src=\"auto\").text\n",
            "\n",
            "\n",
            "destinations = {\n",
            "    \"en\": \"english\",\n",
            "    \"es\": \"spanish\",\n",
            "    \"de\": \"german\",\n",
            "    # \"lt\": \"lithuanian\",\n",
            "    \"ru\": \"russian\",\n",
            "    # \"hu\": \"hungarian\",\n",
            "    \"it\": \"italian\",\n",
            "    \"fr\": \"french\",\n",
            "    # \"sl\": \"slovenian\",\n",
            "    \"no\": \"norwegian\",\n",
            "}\n",
            "translated = []\n",
            "for des in destinations.keys():\n",
            "    translated.append(translate(text, des))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 38,
         "id": "5383beaf",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:06:19.192197Z",
               "iopub.status.busy": "2025-07-04T16:06:19.191973Z",
               "iopub.status.idle": "2025-07-04T16:06:19.197319Z",
               "shell.execute_reply": "2025-07-04T16:06:19.196698Z",
               "shell.execute_reply.started": "2025-07-04T16:06:19.192181Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "['The glass has a capacity of 250 ml for me.To fry pancakes, I used a wide pan to pancakes: diameter 24 cm.I got 16 very thin pancakes from the amount of ingredients given.Take the eggs from the fridge beforehand.You can also lightly heat the milk to a temperature not higher than 40 degrees.Calories were counted on the basis of the ingredients used by me.So this is an approximate amount of calories, because even flour can have a different amount of calories than the one I used.I prepare pancakes at least once a week.It is a simple dish, fast and loved by children.You can serve them for any meal during the day.You can take pancakes to work, school or for a picnic.They are simply divine!',\n",
                     " 'El vidrio tiene una capacidad de 250 ml para mí.Para freír los panqueques, usé una sartén ancha a los panqueques: diámetro de 24 cm.Obtuve 16 panqueques muy delgados por la cantidad de ingredientes dados.Tome los huevos de la nevera de antemano.También puede calentar ligeramente la leche a una temperatura no superior a 40 grados.Las calorías se contaron sobre la base de los ingredientes utilizados por mí.Entonces, esta es una cantidad aproximada de calorías, porque incluso la harina puede tener una cantidad diferente de calorías que la que usé.Preparo panqueques al menos una vez a la semana.Es un plato simple, rápido y amado por los niños.Puede servirlos para cualquier comida durante el día.Puedes llevar panqueques al trabajo, la escuela o para un picnic.¡Son simplemente divinos!',\n",
                     " 'Das Glas hat für mich eine Kapazität von 250 ml. Um Pfannkuchen zu schämen, benutzte ich eine breite Pfanne, um Pfannkuchen zu verbringen: Durchmesser 24 cm.Ich bekam 16 sehr dünne Pfannkuchen aus der Menge an Zutaten.Nehmen Sie die Eier im Voraus aus dem Kühlschrank.Sie können die Milch auch leicht auf eine Temperatur von nicht mehr als 40 Grad erhitzen.Die Kalorien wurden auf der Grundlage der von mir verwendeten Zutaten gezählt.Dies ist also eine ungefähre Menge an Kalorien, da selbst Mehl eine andere Menge an Kalorien haben kann als die, die ich verwendet habe.Ich bereite mindestens einmal pro Woche Pfannkuchen vor.Es ist ein einfaches Gericht, schnell und von Kindern geliebt.Sie können sie tagsüber für jede Mahlzeit servieren.Sie können Pfannkuchen zur Arbeit, Schule oder ein Picknick nehmen.Sie sind einfach göttlich!',\n",
                     " 'Стекло имеет для меня емкость 250 мл.Чтобы жарить блины, я использовал широкую кастрюлю для блинов: диаметр 24 см. Я получил 16 очень тонких блинов от количества данных ингредиентов.Забегайте яйца из холодильника заранее.Вы также можете слегка нагреть молоко до температуры не выше 40 градусов.Калории учитывались на основе используемых мной ингредиентов.Таким образом, это приблизительное количество калорий, потому что даже мука может иметь другое количество калорий, чем то, что я использовал.Я готовлю блины хотя бы раз в неделю.Это простое блюдо, быстро и любимое детьми.Вы можете подавать их для любой еды в течение дня.Вы можете взять блины на работу, школу или для пикника.Они просто божественны!',\n",
                     " 'Il vetro ha una capacità di 250 ml per me.Per friggere i pancake, ho usato una padella ampia per i pancake: diametro 24 cm.Ho ricevuto 16 frittelle molto sottili dalla quantità di ingredienti dati.Prendi in anticipo le uova dal frigorifero.Puoi anche riscaldare leggermente il latte a una temperatura non superiore a 40 gradi.Le calorie sono state contate sulla base degli ingredienti usati da me.Quindi questa è una quantità approssimativa di calorie, perché anche la farina può avere una quantità diversa di calorie da quella che ho usato.Preparo i pancake almeno una volta alla settimana.È un piatto semplice, veloce e amato dai bambini.Puoi servirli per qualsiasi pasto durante il giorno.Puoi prendere i pancake al lavoro, a scuola o per un picnic.Sono semplicemente divini!',\n",
                     " \"Le verre a une capacité de 250 ml pour moi.Pour faire frire les crêpes, j'ai utilisé une large poêle pour les crêpes: diamètre 24 cm.J'ai obtenu 16 crêpes très minces de la quantité d'ingrédients donnés.Prenez les œufs du réfrigérateur à l'avance.Vous pouvez également chauffer légèrement le lait à une température pas supérieure à 40 degrés.Les calories ont été comptées sur la base des ingrédients utilisés par moi.Il s'agit donc d'une quantité approximative de calories, car même la farine peut avoir une quantité différente de calories de celle que j'ai utilisée.Je prépare des crêpes au moins une fois par semaine.C'est un plat simple, rapide et aimé des enfants.Vous pouvez les servir à n'importe quel repas pendant la journée.Vous pouvez prendre des crêpes au travail, à l'école ou pour un pique-nique.Ils sont simplement divins!\",\n",
                     " 'Glasset har en kapasitet på 250 ml for meg.For å steke pannekaker brukte jeg en bred panne til pannekaker: diameter 24 cm.Jeg fikk 16 veldig tynne pannekaker fra mengden ingredienser som er gitt.Ta eggene fra kjøleskapet på forhånd.Du kan også varme opp melken lett til en temperatur som ikke er høyere enn 40 grader.Kalorier ble talt på grunnlag av ingrediensene som ble brukt av meg.Så dette er en omtrentlig mengde kalorier, fordi selv mel kan ha en annen mengde kalorier enn den jeg brukte.Jeg forbereder pannekaker minst en gang i uken.Det er en enkel rett, rask og elsket av barn.Du kan servere dem til ethvert måltid på dagtid.Du kan ta pannekaker på jobb, skole eller for en piknik.De er rett og slett guddommelige!']"
                  ]
               },
               "execution_count": 38,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "translated"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 39,
         "id": "99b62da3",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:06:19.198923Z",
               "iopub.status.busy": "2025-07-04T16:06:19.198067Z",
               "iopub.status.idle": "2025-07-04T16:06:20.173724Z",
               "shell.execute_reply": "2025-07-04T16:06:20.173155Z",
               "shell.execute_reply.started": "2025-07-04T16:06:19.198898Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "import nltk\n",
            "from nltk.tokenize import word_tokenize\n",
            "import random\n",
            "\n",
            "nltk.download(\"punkt_tab\", quiet=True)\n",
            "random.seed(42)\n",
            "\n",
            "\n",
            "def safe_tokenize(text, language, verbose=False) -> list[str]:\n",
            "    try:\n",
            "        return word_tokenize(text, language=language)\n",
            "    except Exception:\n",
            "        if verbose:\n",
            "            print(\n",
            "                f\"nltk tokenization possible for language [{language}]... Defaulting to whitespace tokenization\"\n",
            "            )\n",
            "        return text.split(\" \")\n",
            "\n",
            "\n",
            "tokenized = [\n",
            "    safe_tokenize(t, lan, verbose=True)\n",
            "    for t, lan in zip(translated, destinations.values())\n",
            "]\n",
            "\n",
            "# replace random words with __MASKNOTTRANSLATED__\n",
            "mask = \"<mask>\"\n",
            "\n",
            "\n",
            "def apply_mask(text: list[str], num_masks=5):\n",
            "    all_indicies = set(list(range(0, len(text))))  # so indexes will be unique\n",
            "    for _ in range(num_masks):\n",
            "        idx = random.choice(list(all_indicies))\n",
            "        text[idx] = mask\n",
            "        all_indicies -= {idx}\n",
            "    return text\n",
            "\n",
            "\n",
            "masked_splitted = []\n",
            "for t in tokenized:\n",
            "    masked_splitted.append(apply_mask(t))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 40,
         "id": "f625d5ee",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:06:20.174619Z",
               "iopub.status.busy": "2025-07-04T16:06:20.174426Z",
               "iopub.status.idle": "2025-07-04T16:06:20.179249Z",
               "shell.execute_reply": "2025-07-04T16:06:20.178447Z",
               "shell.execute_reply.started": "2025-07-04T16:06:20.174603Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['The', 'glass', 'has', 'a', 'capacity', 'of', '<mask>', 'ml', 'for', 'me.To', 'fry', 'pancakes', ',', 'I', 'used', 'a', 'wide', 'pan', 'to', 'pancakes', ':', 'diameter', '24', 'cm.I', 'got', '16', 'very', 'thin', '<mask>', 'from', '<mask>', 'amount', 'of', '<mask>', 'given.Take', 'the', 'eggs', 'from', 'the', 'fridge', 'beforehand.You', 'can', 'also', 'lightly', 'heat', 'the', 'milk', 'to', 'a', 'temperature', 'not', 'higher', 'than', '40', 'degrees.Calories', 'were', 'counted', 'on', 'the', 'basis', 'of', 'the', 'ingredients', 'used', 'by', 'me.So', 'this', 'is', 'an', 'approximate', 'amount', 'of', '<mask>', ',', 'because', 'even', 'flour', 'can', 'have', 'a', 'different', 'amount', 'of', 'calories', 'than', 'the', 'one', 'I', 'used.I', 'prepare', 'pancakes', 'at', 'least', 'once', 'a', 'week.It', 'is', 'a', 'simple', 'dish', ',', 'fast', 'and', 'loved', 'by', 'children.You', 'can', 'serve', 'them', 'for', 'any', 'meal', 'during', 'the', 'day.You', 'can', 'take', 'pancakes', 'to', 'work', ',', 'school', 'or', 'for', 'a', 'picnic.They', 'are', 'simply', 'divine', '!']\n",
                  "['El', 'vidrio', 'tiene', 'una', '<mask>', 'de', '250', 'ml', 'para', 'mí.Para', 'freír', 'los', 'panqueques', ',', 'usé', 'una', 'sartén', 'ancha', 'a', 'los', 'panqueques', ':', '<mask>', 'de', '24', 'cm.Obtuve', '<mask>', 'panqueques', 'muy', 'delgados', 'por', 'la', 'cantidad', 'de', 'ingredientes', '<mask>', 'los', 'huevos', 'de', 'la', 'nevera', 'de', 'antemano.También', 'puede', 'calentar', 'ligeramente', 'la', 'leche', 'a', 'una', 'temperatura', 'no', 'superior', 'a', '40', 'grados.Las', 'calorías', 'se', 'contaron', 'sobre', 'la', 'base', 'de', 'los', 'ingredientes', 'utilizados', 'por', 'mí.Entonces', ',', 'esta', 'es', 'una', 'cantidad', 'aproximada', 'de', 'calorías', ',', 'porque', 'incluso', 'la', 'harina', 'puede', 'tener', 'una', 'cantidad', 'diferente', 'de', 'calorías', 'que', 'la', 'que', 'usé.Preparo', 'panqueques', 'al', 'menos', 'una', 'vez', 'a', 'la', 'semana.Es', 'un', 'plato', 'simple', ',', 'rápido', 'y', 'amado', 'por', 'los', 'niños.Puede', 'servirlos', '<mask>', 'cualquier', 'comida', 'durante', 'el', 'día.Puedes', 'llevar', 'panqueques', 'al', 'trabajo', ',', 'la', 'escuela', 'o', 'para', 'un', 'picnic.¡Son', 'simplemente', 'divinos', '!']\n",
                  "['Das', 'Glas', 'hat', 'für', 'mich', 'eine', 'Kapazität', '<mask>', '250', 'ml', '.', 'Um', 'Pfannkuchen', 'zu', 'schämen', ',', 'benutzte', 'ich', 'eine', 'breite', 'Pfanne', ',', 'um', 'Pfannkuchen', '<mask>', 'verbringen', ':', 'Durchmesser', '24', 'cm.Ich', 'bekam', '16', 'sehr', 'dünne', 'Pfannkuchen', 'aus', 'der', 'Menge', 'an', 'Zutaten.Nehmen', 'Sie', 'die', 'Eier', 'im', 'Voraus', 'aus', 'dem', 'Kühlschrank.Sie', 'können', 'die', 'Milch', 'auch', 'leicht', 'auf', 'eine', 'Temperatur', 'von', '<mask>', 'mehr', 'als', '40', 'Grad', '<mask>', 'Kalorien', 'wurden', 'auf', 'der', 'Grundlage', 'der', 'von', 'mir', 'verwendeten', 'Zutaten', 'gezählt.Dies', 'ist', 'also', 'eine', 'ungefähre', 'Menge', 'an', 'Kalorien', ',', 'da', 'selbst', 'Mehl', 'eine', 'andere', 'Menge', 'an', 'Kalorien', 'haben', 'kann', 'als', 'die', ',', 'die', 'ich', 'verwendet', 'habe.Ich', 'bereite', 'mindestens', 'einmal', 'pro', 'Woche', 'Pfannkuchen', 'vor.Es', 'ist', 'ein', 'einfaches', 'Gericht', ',', 'schnell', 'und', 'von', 'Kindern', 'geliebt.Sie', 'können', 'sie', 'tagsüber', 'für', 'jede', 'Mahlzeit', 'servieren.Sie', 'können', 'Pfannkuchen', 'zur', 'Arbeit', ',', 'Schule', 'oder', 'ein', 'Picknick', 'nehmen.Sie', '<mask>', 'einfach', 'göttlich', '!']\n",
                  "['Стекло', 'имеет', 'для', '<mask>', 'емкость', '250', 'мл.Чтобы', 'жарить', 'блины', ',', 'я', 'использовал', 'широкую', 'кастрюлю', 'для', 'блинов', ':', 'диаметр', '24', 'см.', 'Я', 'получил', '16', 'очень', 'тонких', 'блинов', '<mask>', 'количества', 'данных', 'ингредиентов.Забегайте', 'яйца', 'из', 'холодильника', 'заранее.Вы', 'также', 'можете', 'слегка', 'нагреть', 'молоко', 'до', 'температуры', 'не', 'выше', '40', 'градусов.Калории', 'учитывались', 'на', 'основе', 'используемых', 'мной', 'ингредиентов.Таким', 'образом', ',', 'это', 'приблизительное', 'количество', 'калорий', ',', 'потому', 'что', 'даже', 'мука', 'может', 'иметь', 'другое', 'количество', 'калорий', ',', 'чем', 'то', ',', 'что', '<mask>', 'использовал.Я', 'готовлю', 'блины', 'хотя', '<mask>', 'раз', 'в', 'неделю.Это', 'простое', 'блюдо', ',', 'быстро', 'и', 'любимое', 'детьми.Вы', 'можете', 'подавать', 'их', 'для', 'любой', 'еды', 'в', '<mask>', 'дня.Вы', 'можете', 'взять', 'блины', 'на', 'работу', ',', 'школу', 'или', 'для', 'пикника.Они', 'просто', 'божественны', '!']\n",
                  "['Il', 'vetro', 'ha', 'una', 'capacità', 'di', '250', 'ml', 'per', 'me.Per', 'friggere', 'i', 'pancake', ',', 'ho', 'usato', 'una', 'padella', 'ampia', 'per', 'i', 'pancake', ':', 'diametro', '24', 'cm.Ho', 'ricevuto', '16', '<mask>', 'molto', 'sottili', 'dalla', 'quantità', 'di', 'ingredienti', 'dati.Prendi', '<mask>', 'anticipo', 'le', 'uova', 'dal', 'frigorifero.Puoi', 'anche', 'riscaldare', 'leggermente', 'il', 'latte', 'a', 'una', 'temperatura', 'non', 'superiore', 'a', '40', 'gradi.Le', 'calorie', 'sono', 'state', '<mask>', 'sulla', 'base', 'degli', 'ingredienti', 'usati', 'da', 'me.Quindi', 'questa', 'è', 'una', 'quantità', 'approssimativa', 'di', 'calorie', ',', 'perché', 'anche', 'la', '<mask>', 'può', 'avere', 'una', 'quantità', 'diversa', 'di', 'calorie', 'da', 'quella', 'che', 'ho', 'usato.Preparo', 'i', 'pancake', 'almeno', 'una', 'volta', 'alla', 'settimana.È', 'un', 'piatto', 'semplice', ',', 'veloce', 'e', 'amato', 'dai', 'bambini.Puoi', 'servirli', '<mask>', 'qualsiasi', 'pasto', 'durante', 'il', 'giorno.Puoi', 'prendere', 'i', 'pancake', 'al', 'lavoro', ',', 'a', 'scuola', 'o', 'per', 'un', 'picnic.Sono', 'semplicemente', 'divini', '!']\n",
                  "['Le', '<mask>', 'a', 'une', 'capacité', 'de', '250', 'ml', 'pour', 'moi.Pour', 'faire', 'frire', 'les', 'crêpes', ',', \"j'ai\", 'utilisé', 'une', 'large', 'poêle', 'pour', 'les', 'crêpes', ':', 'diamètre', '24', \"cm.J'ai\", 'obtenu', '16', 'crêpes', 'très', 'minces', 'de', 'la', 'quantité', \"d'ingrédients\", 'donnés.Prenez', 'les', 'œufs', 'du', 'réfrigérateur', '<mask>', \"l'avance.Vous\", 'pouvez', 'également', 'chauffer', 'légèrement', 'le', 'lait', 'à', 'une', 'température', 'pas', 'supérieure', 'à', '40', 'degrés.Les', 'calories', 'ont', 'été', 'comptées', 'sur', 'la', 'base', 'des', 'ingrédients', 'utilisés', 'par', 'moi.Il', \"s'agit\", 'donc', \"d'une\", 'quantité', '<mask>', 'de', 'calories', ',', 'car', 'même', 'la', 'farine', 'peut', 'avoir', 'une', 'quantité', 'différente', 'de', 'calories', 'de', '<mask>', 'que', \"j'ai\", 'utilisée.Je', 'prépare', 'des', 'crêpes', 'au', 'moins', 'une', 'fois', 'par', \"semaine.C'est\", 'un', 'plat', 'simple', ',', 'rapide', 'et', 'aimé', 'des', '<mask>', 'pouvez', 'les', 'servir', 'à', \"n'importe\", 'quel', 'repas', 'pendant', 'la', 'journée.Vous', 'pouvez', 'prendre', 'des', 'crêpes', 'au', 'travail', ',', 'à', \"l'école\", 'ou', 'pour', 'un', 'pique-nique.Ils', 'sont', 'simplement', 'divins', '!']\n",
                  "['Glasset', 'har', 'en', 'kapasitet', 'på', '250', 'ml', 'for', 'meg.For', 'å', 'steke', 'pannekaker', 'brukte', 'jeg', 'en', 'bred', 'panne', 'til', 'pannekaker', ':', 'diameter', '24', 'cm.Jeg', 'fikk', '16', 'veldig', 'tynne', '<mask>', 'fra', 'mengden', 'ingredienser', 'som', 'er', 'gitt.Ta', 'eggene', 'fra', 'kjøleskapet', 'på', 'forhånd.Du', '<mask>', 'også', 'varme', 'opp', 'melken', 'lett', '<mask>', 'en', 'temperatur', 'som', 'ikke', 'er', 'høyere', 'enn', '40', 'grader.Kalorier', 'ble', 'talt', 'på', 'grunnlag', 'av', 'ingrediensene', 'som', 'ble', 'brukt', 'av', 'meg.Så', 'dette', 'er', 'en', 'omtrentlig', 'mengde', 'kalorier', ',', 'fordi', 'selv', 'mel', 'kan', 'ha', 'en', 'annen', 'mengde', 'kalorier', 'enn', 'den', 'jeg', 'brukte.Jeg', 'forbereder', 'pannekaker', 'minst', 'en', 'gang', 'i', 'uken.Det', 'er', 'en', 'enkel', 'rett', ',', 'rask', '<mask>', 'elsket', 'av', 'barn.Du', 'kan', 'servere', 'dem', 'til', 'ethvert', 'måltid', 'på', 'dagtid.Du', 'kan', 'ta', 'pannekaker', 'på', 'jobb', ',', 'skole', 'eller', 'for', 'en', 'piknik.De', 'er', 'rett', '<mask>', 'slett', 'guddommelige', '!']\n"
               ]
            }
         ],
         "source": [
            "for masked in masked_splitted:\n",
            "    print(masked)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "37607969",
         "metadata": {},
         "source": [
            "### Masking, Translation and Recreation of masked tokens"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 41,
         "id": "a2d3e515",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:06:20.180759Z",
               "iopub.status.busy": "2025-07-04T16:06:20.180134Z",
               "iopub.status.idle": "2025-07-04T16:06:23.661659Z",
               "shell.execute_reply": "2025-07-04T16:06:23.660905Z",
               "shell.execute_reply.started": "2025-07-04T16:06:20.180734Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
                  "To disable this warning, you can either:\n",
                  "\t- Avoid using `tokenizers` before the fork if possible\n",
                  "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
               ]
            }
         ],
         "source": [
            "# some additional dependencies\n",
            "!pip install -q protobuf sacremoses"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 42,
         "id": "497419d5",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:06:23.662949Z",
               "iopub.status.busy": "2025-07-04T16:06:23.662690Z",
               "iopub.status.idle": "2025-07-04T16:06:23.667920Z",
               "shell.execute_reply": "2025-07-04T16:06:23.667359Z",
               "shell.execute_reply.started": "2025-07-04T16:06:23.662915Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "def fill_all_masks(sentence, mask_pipeline):\n",
            "    placeholder = \"UNIQUE_MASK_PLACEHOLDER\"\n",
            "    while mask in sentence:\n",
            "        parts = sentence.split(mask, 1)\n",
            "        rest = parts[1].replace(mask, placeholder)\n",
            "        sentence_single_mask = parts[0] + mask + rest\n",
            "\n",
            "        suggestions = mask_pipeline(sentence_single_mask)\n",
            "        best_token = suggestions[0][\"token_str\"]\n",
            "\n",
            "        sentence_filled = sentence_single_mask.replace(mask, best_token, 1)\n",
            "        sentence = sentence_filled.replace(placeholder, mask)\n",
            "    return sentence"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 43,
         "id": "542f05b7",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:06:23.668984Z",
               "iopub.status.busy": "2025-07-04T16:06:23.668760Z",
               "iopub.status.idle": "2025-07-04T16:06:53.797926Z",
               "shell.execute_reply": "2025-07-04T16:06:53.797192Z",
               "shell.execute_reply.started": "2025-07-04T16:06:23.668963Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Device set to use cpu\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\n",
                  "Masked:  Szkło ma dla mnie pojemność 250 ml. Do smażenia naleśników użyłem <mask> patelni <mask> <mask> średnica 24 cm. I dostałem 16 bardzo cienkich naleśników z ilości podanych składników. Wczoraj jaja z lodówki.Może mieć inną ilość kalorii niż ta, której użyłem. Przynajmniej przygotowuję naleśniki przynajmniej raz w tygodniu. To <mask> danie, <mask> i kochane przez dzieci. Możesz podać je na każdy posiłek w ciągu dnia. Możesz zabrać naleśniki do pracy, szkoły lub na piknik. Są po prostu boskie!\n",
                  "Filled:  Szkło ma dla mnie pojemność 250 ml. Do smażenia naleśników użyłem specjalnej patelni , jej średnica 24 cm. I dostałem 16 bardzo cienkich naleśników z ilości podanych składników. Wczoraj jaja z lodówki.Może mieć inną ilość kalorii niż ta, której użyłem. Przynajmniej przygotowuję naleśniki przynajmniej raz w tygodniu. To ulubione danie, popularne i kochane przez dzieci. Możesz podać je na każdy posiłek w ciągu dnia. Możesz zabrać naleśniki do pracy, szkoły lub na piknik. Są po prostu boskie!\n",
                  "\n",
                  "Masked:  Szkło ma dla mnie pojemność <mask> ml. Aby usmażyć naleśniki, użyłem szerokiej patelni do naleśników: średnica 24 cm. Otchnij 16 naleśników bardzo cienki, jest to przybliżona ilość kalorii, ponieważ nawet mąka może mieć <mask> ilość kalorii niż ta, której użyłem. Preparo Panqueques przynajmniej raz <mask> tygodniu. To proste danie, szybkie i kochane przez dzieci. Może serwować je do każdego jedzenia w <mask> dnia. Możesz zabrać naleśniki do pracy, szkoły lub na piknik. <mask> po prostu boskie!\n",
                  "Filled:  Szkło ma dla mnie pojemność 200 ml. Aby usmażyć naleśniki, użyłem szerokiej patelni do naleśników: średnica 24 cm. Otchnij 16 naleśników bardzo cienki, jest to przybliżona ilość kalorii, ponieważ nawet mąka może mieć większą ilość kalorii niż ta, której użyłem. Preparo Panqueques przynajmniej raz w tygodniu. To proste danie, szybkie i kochane przez dzieci. Może serwować je do każdego jedzenia w ciągu dnia. Możesz zabrać naleśniki do pracy, szkoły lub na piknik. Są po prostu boskie!\n",
                  "\n",
                  "Masked:  Dla mnie szkło ma pojemność 250 ml. Aby wstydzić się <mask> użyłem szerokiej patelni, aby <mask> naleśniki: średnica 24 cm. Dostałem 16 bardzo cienkich naleśników z tłumu składników. Wyciągnij jaja z lodówki z wyprzedzeniem. To się liczy. Jest to <mask> ilość kalorii, ponieważ nawet mąka może mieć inną ilość <mask> niż ta, której użyłem. Przynajmniej raz w tygodniu przygotowuję naleśniki. To proste danie, szybko i kochane przez dzieci. Możesz podawać do każdego posiłku <mask> ciągu dnia.\n",
                  "Filled:  Dla mnie szkło ma pojemność 250 ml. Aby wstydzić się , użyłem szerokiej patelni, aby przygotować naleśniki: średnica 24 cm. Dostałem 16 bardzo cienkich naleśników z tłumu składników. Wyciągnij jaja z lodówki z wyprzedzeniem. To się liczy. Jest to duża ilość kalorii, ponieważ nawet mąka może mieć inną ilość , niż ta, której użyłem. Przynajmniej raz w tygodniu przygotowuję naleśniki. To proste danie, szybko i kochane przez dzieci. Możesz podawać do każdego posiłku w ciągu dnia.\n",
                  "\n",
                  "Masked:  Glass ma dla mnie pojemność 250 ml. Do smażenia naleśników użyłem szerokiej patelni na naleśniki: średnicę 24 cm. Otrzymałem 16 bardzo cienkich naleśników z <mask> składników. Wcześniej pieprzyć jajka z lodówki. Możesz również lekko podgrzać mleko do temperatury nie wyższej niż 40 stopni. Kalorie wzięto pod <mask> na podstawie użytych składników. Mąka może mieć inną liczbę kalorii niż używałem. Gotuję naleśniki przynajmniej raz w tygodniu. To proste danie, szybko i ukochane przez dzieci. Możesz <mask> je do dowolnego jedzenia <mask> <mask> dnia. Możesz zabrać naleśniki do pracy, szkoły lub na piknik. Są po prostu boskie!\n",
                  "Filled:  Glass ma dla mnie pojemność 250 ml. Do smażenia naleśników użyłem szerokiej patelni na naleśniki: średnicę 24 cm. Otrzymałem 16 bardzo cienkich naleśników z różnych składników. Wcześniej pieprzyć jajka z lodówki. Możesz również lekko podgrzać mleko do temperatury nie wyższej niż 40 stopni. Kalorie wzięto pod uwagę na podstawie użytych składników. Mąka może mieć inną liczbę kalorii niż używałem. Gotuję naleśniki przynajmniej raz w tygodniu. To proste danie, szybko i ukochane przez dzieci. Możesz dodać je do dowolnego jedzenia ( każdego dnia. Możesz zabrać naleśniki do pracy, szkoły lub na piknik. Są po prostu boskie!\n",
                  "\n",
                  "Masked:  Szkło ma dla mnie pojemność <mask> ml. Aby <mask> naleśniki, użyłem dużej patelni do naleśników: średnica 24 cm. Otrzymałem 16 bardzo cienkich naleśników z ilości składników danych. Z góry jaja z <mask> Nawet mleko lekko podgrzewaj mleko w temperaturze nieprzekraczającej 40 stopni. Kalorie zostały zliczone na podstawie składników użytych przeze mnie. Przybliżone kalorie, ponieważ nawet mąka może mieć inną ilość kalorii niż użyłem. Gram naleśniki przynajmniej raz w tygodniu. To proste danie, szybkie i kochane przez dzieci. Możesz podać je na każdy posiłek w ciągu dnia. <mask> naleśniki w pracy, <mask> szkole lub na piknik. Są po prostu boskie!\n",
                  "Filled:  Szkło ma dla mnie pojemność 200 ml. Aby przygotować naleśniki, użyłem dużej patelni do naleśników: średnica 24 cm. Otrzymałem 16 bardzo cienkich naleśników z ilości składników danych. Z góry jaja z dołu Nawet mleko lekko podgrzewaj mleko w temperaturze nieprzekraczającej 40 stopni. Kalorie zostały zliczone na podstawie składników użytych przeze mnie. Przybliżone kalorie, ponieważ nawet mąka może mieć inną ilość kalorii niż użyłem. Gram naleśniki przynajmniej raz w tygodniu. To proste danie, szybkie i kochane przez dzieci. Możesz podać je na każdy posiłek w ciągu dnia. Lubię naleśniki w pracy, w szkole lub na piknik. Są po prostu boskie!\n",
                  "\n",
                  "Masked:  Glass ma dla mnie pojemność 250 ml. Aby usmażyć naleśniki, <mask> dużej patelni <mask> naleśników: średnica 24 cm. Otrzymałem 16 bardzo cienkich naleśników z podanych składników. Z wyprzedzeniem pory lodówki. <mask> również lekko podgrzać mleko w temperaturze <mask> większej niż 40 stopni. Ja. Jest to zatem przybliżona ilość kalorii, ponieważ nawet mąka <mask> mieć inną ilość kalorii tego, którego użyłem. Przynajmniej raz w tygodniu przygotowuję naleśniki. To proste, szybkie i kochane danie. Możesz podawać im na dowolnym posiłku w ciągu dnia. Możesz brać naleśniki w pracy, w szkole lub na piknik. Są po prostu boskie!\n",
                  "Filled:  Glass ma dla mnie pojemność 250 ml. Aby usmażyć naleśniki, potrzebuję dużej patelni do naleśników: średnica 24 cm. Otrzymałem 16 bardzo cienkich naleśników z podanych składników. Z wyprzedzeniem pory lodówki. Można również lekko podgrzać mleko w temperaturze nie większej niż 40 stopni. Ja. Jest to zatem przybliżona ilość kalorii, ponieważ nawet mąka może mieć inną ilość kalorii tego, którego użyłem. Przynajmniej raz w tygodniu przygotowuję naleśniki. To proste, szybkie i kochane danie. Możesz podawać im na dowolnym posiłku w ciągu dnia. Możesz brać naleśniki w pracy, w szkole lub na piknik. Są po prostu boskie!\n",
                  "\n",
                  "Masked:  Szkło ma dla mnie pojemność 250 ml. Do smażenia naleśników użyłem szerokiej patelni do naleśników: średnica 24 cm. Mam 16 <mask> cienkich naleśników z liczby podanych składników. Wybierz jajka z lodówki z wyprzedzeniem. Możesz również <mask> podgrzać mleko do temperatury, która nie jest wyższa niż 40 stopni. <mask> ponieważ nawet mąka może mieć inną ilość kalorii niż ta, której <mask> Przynajmniej raz w tygodniu przygotowuję naleśniki. To proste danie, szybkie i kochane przez dzieci. Możesz podać je na każdy posiłek w ciągu dnia. <mask> wziąć naleśniki w pracy, w szkole lub na piknik.\n",
                  "Filled:  Szkło ma dla mnie pojemność 250 ml. Do smażenia naleśników użyłem szerokiej patelni do naleśników: średnica 24 cm. Mam 16 rodzajów cienkich naleśników z liczby podanych składników. Wybierz jajka z lodówki z wyprzedzeniem. Możesz również spróbować podgrzać mleko do temperatury, która nie jest wyższa niż 40 stopni. A ponieważ nawet mąka może mieć inną ilość kalorii niż ta, której potrzebujemy Przynajmniej raz w tygodniu przygotowuję naleśniki. To proste danie, szybkie i kochane przez dzieci. Możesz podać je na każdy posiłek w ciągu dnia. Możesz wziąć naleśniki w pracy, w szkole lub na piknik.\n"
               ]
            }
         ],
         "source": [
            "from transformers import pipeline\n",
            "from googletrans import Translator\n",
            "\n",
            "mask_pipeline = pipeline(\"fill-mask\", model=\"allegro/herbert-base-cased\")\n",
            "translator = Translator()\n",
            "\n",
            "for foreign_text, lan in zip(translated, destinations.keys()):\n",
            "    # translate to HERBERT native language - polish\n",
            "    pl_text = translator.translate(foreign_text, dest=\"pl\").text\n",
            "    # tokenize\n",
            "    pl_tokens = safe_tokenize(pl_text, language=lan)\n",
            "    # apply mask\n",
            "    masked_tokens = apply_mask(pl_tokens)\n",
            "    # join splitted\n",
            "    masked = \" \".join(masked_tokens)\n",
            "    print(\"\\nMasked: \", masked)\n",
            "\n",
            "    final_sentence = fill_all_masks(masked, mask_pipeline)\n",
            "    print(\"Filled: \", final_sentence)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "9f5c9f74",
         "metadata": {},
         "source": [
            "## Recursive translation"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "id": "a13a7b7e",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:06:53.798913Z",
               "iopub.status.busy": "2025-07-04T16:06:53.798699Z",
               "iopub.status.idle": "2025-07-04T16:06:53.802180Z",
               "shell.execute_reply": "2025-07-04T16:06:53.801444Z",
               "shell.execute_reply.started": "2025-07-04T16:06:53.798897Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "original_text = \"Als Zweiter Weltkrieg wird der zweite global geführte Krieg sämtlicher Großmächte im 20. Jahrhundert bezeichnet. Über 60 Staaten waren direkt oder indirekt beteiligt, mehr als 110 Millionen Menschen trugen Waffen. Schätzungen zufolge wurden über 65 Millionen Menschen getötet.\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "id": "ca845c8d",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:06:53.803080Z",
               "iopub.status.busy": "2025-07-04T16:06:53.802903Z",
               "iopub.status.idle": "2025-07-04T16:06:53.817741Z",
               "shell.execute_reply": "2025-07-04T16:06:53.816893Z",
               "shell.execute_reply.started": "2025-07-04T16:06:53.803066Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "destinations = {\n",
            "    \"en\": \"english\",\n",
            "    \"es\": \"spanish\",\n",
            "    \"lt\": \"lithuanian\",\n",
            "    \"ru\": \"russian\",\n",
            "    \"hu\": \"hungarian\",\n",
            "    \"it\": \"italian\",\n",
            "    \"fr\": \"french\",\n",
            "    \"sl\": \"slovenian\",\n",
            "    \"no\": \"norwegian\",\n",
            "    \"de\": \"german\",\n",
            "}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "id": "d70723b6",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:06:53.818893Z",
               "iopub.status.busy": "2025-07-04T16:06:53.818577Z",
               "iopub.status.idle": "2025-07-04T16:07:17.666650Z",
               "shell.execute_reply": "2025-07-04T16:07:17.665939Z",
               "shell.execute_reply.started": "2025-07-04T16:06:53.818876Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Translating from 'de' to 'en'\n",
                  "    Translated: The second global war of all major powers in the 20th century was called the Second World War.Over 60 states were directly or indirectly involved, more than 110 million people wore weapons.It is estimated that over 65 million people were killed.\n",
                  "    Levenshtein edit distance: 183\n",
                  "Translating from 'en' to 'es'\n",
                  "    Translated: La Segunda Guerra Global de todas las potencias importantes en el siglo XX se llamó la Segunda Guerra Mundial. Over 60 estados estaban directa o indirectamente involucrados, más de 110 millones de personas llevaban armas. Se estima que más de 65 millones de personas fueron asesinadas.\n",
                  "    Levenshtein edit distance: 207\n",
                  "Translating from 'es' to 'lt'\n",
                  "    Translated: Antrasis visų svarbių XX amžiaus galių karas buvo vadinamas Antrojo pasaulinio karo.Daugiau nei 60 valstijų tiesiogiai ar netiesiogiai dalyvavo, daugiau nei 110 milijonų žmonių nešiojo ginklus.Manoma, kad žuvo daugiau nei 65 milijonai žmonių.\n",
                  "    Levenshtein edit distance: 217\n",
                  "Translating from 'lt' to 'ru'\n",
                  "    Translated: Вторая война всех важных сил 20 -го века была названа Второй мировой войной. Более 110 миллионов человек выполняли оружие напрямую или косвенно.\n",
                  "    Levenshtein edit distance: 249\n",
                  "Translating from 'ru' to 'hu'\n",
                  "    Translated: A 20. század minden fontos erõjének második háborúját második világháborúnak hívták.Több mint 110 millió ember hajtott végre fegyvereket közvetlenül vagy közvetve.\n",
                  "    Levenshtein edit distance: 213\n",
                  "Translating from 'hu' to 'it'\n",
                  "    Translated: La seconda guerra di tutte le forze importanti del 20 ° secolo fu chiamata Seconda Guerra Mondiale. Più di 110 milioni di persone hanno realizzato armi direttamente o indirettamente.\n",
                  "    Levenshtein edit distance: 202\n",
                  "Translating from 'it' to 'fr'\n",
                  "    Translated: La deuxième guerre de toutes les forces importantes du 20e siècle a été appelée la Seconde Guerre mondiale.Plus de 110 millions de personnes ont fabriqué des armes directement ou indirectement.\n",
                  "    Levenshtein edit distance: 199\n",
                  "Translating from 'fr' to 'sl'\n",
                  "    Translated: Druga vojna vseh pomembnih sil 20. stoletja se je imenovala druga svetovna vojna. V 110 milijonov ljudi so orožje izdelovali neposredno ali posredno.\n",
                  "    Levenshtein edit distance: 214\n",
                  "Translating from 'sl' to 'no'\n",
                  "    Translated: Den andre krigen for alle viktige krefter på 1900 -tallet ble kalt andre verdenskrig.Hos 110 millioner mennesker ble det gjort våpen direkte eller indirekte.\n",
                  "    Levenshtein edit distance: 193\n",
                  "Translating from 'no' to 'de'\n",
                  "    Translated: Der zweite Krieg für alle wichtigen Mächte in den 1900er Jahren wurde als Zweiten Weltkrieg bezeichnet. Hos 110 Millionen Menschen wurden direkt oder indirekt gemacht.\n",
                  "    Levenshtein edit distance: 176\n"
               ]
            }
         ],
         "source": [
            "from googletrans import Translator\n",
            "from nltk.metrics import edit_distance\n",
            "import copy\n",
            "\n",
            "translator = Translator()\n",
            "translated = copy.copy(original_text)\n",
            "current_language = \"de\"\n",
            "for lan in destinations.keys():\n",
            "    print(f\"Translating from '{current_language}' to '{lan}'\")\n",
            "    translated = translator.translate(translated, dest=lan, src=\"auto\").text\n",
            "    print(f\"    Translated: {translated}\")\n",
            "\n",
            "    distance = edit_distance(\n",
            "        original_text, translated, substitution_cost=1, transpositions=False\n",
            "    )\n",
            "    print(f\"    Levenshtein edit distance: {distance}\")\n",
            "    current_language = lan"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "b518e1a6",
         "metadata": {},
         "source": [
            "### How far is the translation and the original text?"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "id": "0872a0c8",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:07:17.668157Z",
               "iopub.status.busy": "2025-07-04T16:07:17.667934Z",
               "iopub.status.idle": "2025-07-04T16:07:17.699477Z",
               "shell.execute_reply": "2025-07-04T16:07:17.698748Z",
               "shell.execute_reply.started": "2025-07-04T16:07:17.668139Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Original: \n",
                  "Als Zweiter Weltkrieg wird der zweite global geführte Krieg sämtlicher Großmächte im 20. Jahrhundert bezeichnet. Über 60 Staaten waren direkt oder indirekt beteiligt, mehr als 110 Millionen Menschen trugen Waffen. Schätzungen zufolge wurden über 65 Millionen Menschen getötet.\n",
                  "Translated: \n",
                  "Der zweite Krieg für alle wichtigen Mächte in den 1900er Jahren wurde als Zweiten Weltkrieg bezeichnet. Hos 110 Millionen Menschen wurden direkt oder indirekt gemacht.\n",
                  "\n",
                  "Levenshtein edit distance: 176\n"
               ]
            }
         ],
         "source": [
            "print(f\"Original: \\n{original_text}\")\n",
            "print(f\"Translated: \\n{translated}\")\n",
            "distance = edit_distance(\n",
            "    original_text, translated, substitution_cost=1, transpositions=False\n",
            ")\n",
            "print(f\"\\nLevenshtein edit distance: {distance}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a8616239",
         "metadata": {},
         "source": [
            "The score is high which means there are significant differences between original and multi-tranlated text."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "d41cfd81",
         "metadata": {},
         "source": [
            "## Translating pdf file"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "id": "31f1ffa2",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:07:17.700514Z",
               "iopub.status.busy": "2025-07-04T16:07:17.700267Z",
               "iopub.status.idle": "2025-07-04T16:07:22.809696Z",
               "shell.execute_reply": "2025-07-04T16:07:22.808928Z",
               "shell.execute_reply.started": "2025-07-04T16:07:17.700488Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
                  "To disable this warning, you can either:\n",
                  "\t- Avoid using `tokenizers` before the fork if possible\n",
                  "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                  "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                  "\u001b[?25h  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
               ]
            }
         ],
         "source": [
            "!pip install -q PyPDF2 fpdf"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "id": "a7e516ad",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:07:22.811142Z",
               "iopub.status.busy": "2025-07-04T16:07:22.810871Z",
               "iopub.status.idle": "2025-07-04T16:07:24.073186Z",
               "shell.execute_reply": "2025-07-04T16:07:24.072421Z",
               "shell.execute_reply.started": "2025-07-04T16:07:22.811118Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "import PyPDF2\n",
            "from fpdf import FPDF\n",
            "from googletrans import Translator\n",
            "\n",
            "\n",
            "def extract_text(pdf_path):\n",
            "    with open(pdf_path, \"rb\") as f:\n",
            "        reader = PyPDF2.PdfReader(f)\n",
            "        full_text = \"\"\n",
            "        for page in reader.pages:\n",
            "            full_text += page.extract_text() + \"\\n\"\n",
            "    return full_text\n",
            "\n",
            "\n",
            "def translate(text, translate_to=\"de\"):\n",
            "    translator = Translator()\n",
            "    return translator.translate(text, dest=translate_to, src=\"en\").text\n",
            "\n",
            "\n",
            "# 3. Save text to PDF using fpdf (simpler than reportlab)\n",
            "def save_text_to_pdf(text, output_path):\n",
            "    pdf = FPDF()\n",
            "    pdf.add_page()\n",
            "    pdf.set_auto_page_break(auto=True, margin=15)\n",
            "    pdf.set_font(\"Arial\", size=12)\n",
            "    for line in text.split(\"\\n\"):\n",
            "        pdf.cell(0, 10, txt=line, ln=True)\n",
            "    pdf.output(output_path)\n",
            "\n",
            "\n",
            "# Usage\n",
            "input_pdf = \"/kaggle/input/translation/EN-rival600-manu.pdf\"\n",
            "output_pdf = \"/DE-rival600-manu.pdf\"\n",
            "\n",
            "text = extract_text(input_pdf)\n",
            "translated_text = translate(text)\n",
            "save_text_to_pdf(translated_text, output_pdf)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "57eb08ce",
         "metadata": {},
         "source": [
            "If you look at the original file you can see that all syle was lost"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "952b5276",
         "metadata": {},
         "source": [
            "## Translation with context awareness and large documents handling"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "id": "b3f17ac3",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:07:24.074446Z",
               "iopub.status.busy": "2025-07-04T16:07:24.074142Z",
               "iopub.status.idle": "2025-07-04T16:07:24.082128Z",
               "shell.execute_reply": "2025-07-04T16:07:24.081274Z",
               "shell.execute_reply.started": "2025-07-04T16:07:24.074425Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "# [x] Find large document\n",
            "# [x] Load document\n",
            "# [x] Read full text\n",
            "# [x] Merge text\n",
            "# [x] Split into 10 sentenses long chunks and save 3 previous and 3 following sentences as context\n",
            "# [x] Use mBART to translate with surronding context\n",
            "# [] Save results in 2 markdown files. One for each language. Each sentence in one\n",
            "from nltk.tokenize import PunktSentenceTokenizer\n",
            "\n",
            "\n",
            "class Chunk:\n",
            "    def __init__(self, core, previous=\"\", following=\"\"):\n",
            "        self.previous: str = previous\n",
            "        self.core: str = core\n",
            "        self.following: str = following\n",
            "\n",
            "    @staticmethod\n",
            "    def from_raw_text(corpus: str, chunk_size: int, context_size: int):\n",
            "        # assert chunk_size > context_size, \"Chunk size must be grater than context\"\n",
            "        tokenizer = PunktSentenceTokenizer(corpus)\n",
            "        sentences = tokenizer.tokenize(corpus)\n",
            "\n",
            "        chunks: list[\"Chunk\"] = []\n",
            "\n",
            "        chunks_num = int(len(sentences) / chunk_size)\n",
            "        remaining = len(sentences) - chunks_num\n",
            "\n",
            "        for i in range(chunks_num):\n",
            "            start_idx = i * chunk_size\n",
            "            core = sentences[start_idx : start_idx + chunk_size]\n",
            "            previous_context = (\n",
            "                sentences[start_idx - context_size : start_idx] if i != 0 else [\"\"]\n",
            "            )\n",
            "            following_context = (\n",
            "                sentences[\n",
            "                    start_idx + chunk_size : start_idx + chunk_size + context_size\n",
            "                ]\n",
            "                if i < chunks_num\n",
            "                else sentences[start_idx + context_size : len(sentences)]\n",
            "            )\n",
            "\n",
            "            chunks.append(\n",
            "                Chunk(\n",
            "                    Chunk.join_str(core),\n",
            "                    Chunk.join_str(previous_context),\n",
            "                    Chunk.join_str(following_context),\n",
            "                )\n",
            "            )\n",
            "\n",
            "        if remaining:\n",
            "            previous_context = chunks[-1].following\n",
            "            core = sentences[-3:]\n",
            "\n",
            "            chunks.append(\n",
            "                Chunk(\n",
            "                    core=core, previous=Chunk.join_str(previous_context), following=\"\"\n",
            "                )\n",
            "            )\n",
            "\n",
            "        return chunks\n",
            "\n",
            "    def __repr__(self) -> str:\n",
            "        return f\"{60*'='}\\nCORE: {self.core}\\nPREVIOUS: {self.previous}\\nFOLLOWING: {self.following}\\n{60*'='}\"\n",
            "\n",
            "    @staticmethod\n",
            "    def join_str(splitted: list[str]):\n",
            "        return \" \".join(splitted)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2e879009",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:07:24.083286Z",
               "iopub.status.busy": "2025-07-04T16:07:24.083028Z",
               "iopub.status.idle": "2025-07-04T16:07:24.178910Z",
               "shell.execute_reply": "2025-07-04T16:07:24.178278Z",
               "shell.execute_reply.started": "2025-07-04T16:07:24.083264Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[============================================================\n",
                     " CORE: Obcy, ciągle w\n",
                     " płaszczu, stał przed szynkwasem sztywno, nieruchomo, milczał. – Co podać? – Piwa – rzekł nieznajomy.\n",
                     " PREVIOUS: Karczma nie miała najlepszej sławy. Karczmarz uniósł głowę znad beczki kiszonych ogórków i zmierzył gościa wzrokiem.\n",
                     " FOLLOWING: Głos miał nieprzyjemny. Karczmarz wytarł ręce o płócienny fartuch i\n",
                     " napełnił gliniany kufel.\n",
                     " ============================================================,\n",
                     " ============================================================\n",
                     " CORE: Głos miał nieprzyjemny. Karczmarz wytarł ręce o płócienny fartuch i\n",
                     " napełnił gliniany kufel. Kufel był wyszczerbiony.\n",
                     " PREVIOUS: – Co podać? – Piwa – rzekł nieznajomy.\n",
                     " FOLLOWING: Nieznajomy nie był stary, ale włosy miał prawie zupełnie białe. Pod płaszczem nosił wytarty\n",
                     " skórzany kubrak, sznurowany pod szyją i na ramionach.\n",
                     " ============================================================]"
                  ]
               },
               "execution_count": 23,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "from pathlib import Path\n",
            "\n",
            "text = Path(\"ostatnie_życzenie.txt\").read_text()\n",
            "\n",
            "text_chunks = Chunk.from_raw_text(text, 3, 2)\n",
            "text_chunks[10:12]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "id": "bad5dd32-fbd6-4a8f-9a9e-e34b3f8acbe7",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:07:24.180109Z",
               "iopub.status.busy": "2025-07-04T16:07:24.179847Z",
               "iopub.status.idle": "2025-07-04T16:07:24.184143Z",
               "shell.execute_reply": "2025-07-04T16:07:24.183439Z",
               "shell.execute_reply.started": "2025-07-04T16:07:24.180091Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "import os\n",
            "\n",
            "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
            "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "48018c02",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:07:24.185076Z",
               "iopub.status.busy": "2025-07-04T16:07:24.184896Z",
               "iopub.status.idle": "2025-07-04T16:07:51.063459Z",
               "shell.execute_reply": "2025-07-04T16:07:51.062852Z",
               "shell.execute_reply.started": "2025-07-04T16:07:24.185062Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "from transformers import AutoTokenizer, MBartForConditionalGeneration\n",
            "import uuid\n",
            "import torch\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
            "model = MBartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
            "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            "tokenizer.src_lang = \"pl_XX\"\n",
            "\n",
            "\n",
            "def prepare_prompt(chunk: Chunk, start_id: str, end_id: str) -> str:\n",
            "    return f\"Konteks początkowy: {chunk.previous} Tekst: {start_id}{chunk.core}{end_id} Kontekst końcowy: {chunk.following}\"\n",
            "\n",
            "\n",
            "def translate_chunks_in_batch(chunks: list[Chunk], batch_size: int = 8) -> list[str]:\n",
            "    results = []\n",
            "\n",
            "    for i in range(0, len(chunks), batch_size):\n",
            "        batch = chunks[i : i + batch_size]\n",
            "\n",
            "        # create unique id per chunk\n",
            "        batch_ids = [(uuid.uuid4(), uuid.uuid4()) for _ in batch]\n",
            "        prompts = [\n",
            "            prepare_prompt(chunk, str(start_id), str(end_id))\n",
            "            for chunk, (start_id, end_id) in zip(batch, batch_ids)\n",
            "        ]\n",
            "\n",
            "        inputs = tokenizer(\n",
            "            prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024\n",
            "        ).to(device)\n",
            "\n",
            "        generated = model.generate(\n",
            "            **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
            "        )\n",
            "        decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
            "\n",
            "        for text, (start_id, end_id) in zip(decoded, batch_ids):\n",
            "            start_marker = str(start_id).lower()\n",
            "            end_marker = str(end_id).lower()\n",
            "            lower_text = text.lower()\n",
            "            start = lower_text.find(start_marker) + len(start_marker)\n",
            "            end = lower_text.find(end_marker)\n",
            "            results.append(text[start:end])\n",
            "\n",
            "    return results"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "de66fdc5",
         "metadata": {},
         "source": [
            "**I tested multiple variances with multithreading, multiprocessing and bathing and the best found approach is batch translating**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "id": "02bec7f0",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:07:51.064449Z",
               "iopub.status.busy": "2025-07-04T16:07:51.064215Z",
               "iopub.status.idle": "2025-07-04T16:08:33.518441Z",
               "shell.execute_reply": "2025-07-04T16:08:33.517850Z",
               "shell.execute_reply.started": "2025-07-04T16:07:51.064431Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "text_chunks = text_chunks[:50]  # take few first chunks\n",
            "translations = translate_chunks_in_batch(text_chunks, batch_size=8)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "id": "b6622ad7-3e28-4190-a1e3-a76ff797aae3",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:08:33.519389Z",
               "iopub.status.busy": "2025-07-04T16:08:33.519189Z",
               "iopub.status.idle": "2025-07-04T16:08:33.524802Z",
               "shell.execute_reply": "2025-07-04T16:08:33.523965Z",
               "shell.execute_reply.started": "2025-07-04T16:08:33.519373Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "translations = translations\n",
            "originals = [c.core for c in text_chunks]\n",
            "\n",
            "with open(\"translation_results.md\", \"+w\", encoding=\"utf-8\") as f:\n",
            "    for pl, en in zip(originals, translations):\n",
            "        f.write(f\"### PL:\\n{pl.strip()}\\n\\n\")\n",
            "        f.write(f\"### EN:\\n{en.strip()}\\n\\n\")\n",
            "        f.write(\"---\\n\\n\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "6cfa1fc2",
         "metadata": {},
         "source": [
            "## Translation with tone adjustment and sentiment analysis"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "id": "90e83ba0-9259-4491-8ad6-c8c32bc7f638",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:08:33.528791Z",
               "iopub.status.busy": "2025-07-04T16:08:33.528065Z",
               "iopub.status.idle": "2025-07-04T16:08:33.546428Z",
               "shell.execute_reply": "2025-07-04T16:08:33.545715Z",
               "shell.execute_reply.started": "2025-07-04T16:08:33.528768Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "formal_letter = \"\"\"Dear Sir or Madam,\n",
            "I am writing to formally express my interest in the Data Analyst position at your esteemed organization. With a solid academic background in data science and hands-on experience in statistical modeling, data visualization, and machine learning, I believe I possess the qualifications necessary to contribute meaningfully to your team. My previous role involved designing and implementing data pipelines, generating actionable insights, and supporting key business decisions.\n",
            "I have developed a strong proficiency in tools such as Python, SQL, and Tableau, and I am confident in my ability to adapt quickly to new systems and workflows. I am highly motivated, detail-oriented, and committed to delivering high-quality analytical support.\n",
            "I would welcome the opportunity to further discuss how my skills and experience align with your organization’s goals. Please feel free to contact me at your earliest convenience should you require any additional information or documentation.\n",
            "Thank you for your time and consideration.\"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c355531b-0cdb-45a6-b533-60a410dc32dc",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:08:33.547420Z",
               "iopub.status.busy": "2025-07-04T16:08:33.547169Z",
               "iopub.status.idle": "2025-07-04T16:12:19.461856Z",
               "shell.execute_reply": "2025-07-04T16:12:19.461099Z",
               "shell.execute_reply.started": "2025-07-04T16:08:33.547382Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "!pip install -q huggingface_hub vllm torchvision transformers"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "3385df7b-7096-4c84-aa93-3e9a01024071",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:12:19.463255Z",
               "iopub.status.busy": "2025-07-04T16:12:19.462946Z",
               "iopub.status.idle": "2025-07-04T16:12:19.715752Z",
               "shell.execute_reply": "2025-07-04T16:12:19.715185Z",
               "shell.execute_reply.started": "2025-07-04T16:12:19.463223Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "from huggingface_hub import login\n",
            "import os\n",
            "\n",
            "token = os.getenv(\"HF_TOKEN\")\n",
            "\n",
            "login(token)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "31a873f6-1727-4038-ad15-dbfe64f83de1",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:18:10.030236Z",
               "iopub.status.busy": "2025-07-04T16:18:10.029329Z",
               "iopub.status.idle": "2025-07-04T16:20:04.271114Z",
               "shell.execute_reply": "2025-07-04T16:20:04.270328Z",
               "shell.execute_reply.started": "2025-07-04T16:18:10.030211Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "!pip install -q vllm"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 40,
         "id": "61c6e756-0179-4334-85a7-efaff2de48b5",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:20:44.541259Z",
               "iopub.status.busy": "2025-07-04T16:20:44.540723Z",
               "iopub.status.idle": "2025-07-04T16:20:44.678973Z",
               "shell.execute_reply": "2025-07-04T16:20:44.677802Z",
               "shell.execute_reply.started": "2025-07-04T16:20:44.541232Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "INFO 07-04 16:20:44 [__init__.py:244] Automatically detected platform cuda.\n"
               ]
            },
            {
               "ename": "ImportError",
               "evalue": "/usr/local/lib/python3.11/dist-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c106ivalue14ConstantString6createENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
                  "\u001b[0;32m/tmp/ipykernel_35/3757082513.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSamplingParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"speakleash/Bielik-4.5B-v3.0-Instruct-FP8-Dynamic\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_override\u001b[0m  \u001b[0;31m# isort:skip  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsyncEngineArgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEngineArgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_llm_engine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsyncLLMEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLMEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from vllm.config import (BlockSize, CacheConfig, CacheDType, CompilationConfig,\n\u001b[0m\u001b[1;32m     23\u001b[0m                          \u001b[0mConfigFormat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecodingConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                          \u001b[0mDetailedTraceModules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeviceConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompilation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minductor_pass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallableInductorPass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInductorPass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,\n\u001b[0m\u001b[1;32m     38\u001b[0m                                                      \u001b[0mQuantizationMethods\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                                      get_quantization_config)\n",
                  "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from vllm.model_executor.parameter import (BasevLLMParameter,\n\u001b[0m\u001b[1;32m      5\u001b[0m                                            PackedvLLMParameter)\n\u001b[1;32m      6\u001b[0m from vllm.model_executor.sampling_metadata import (SamplingMetadata,\n",
                  "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/parameter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tensor_model_parallel_rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_make_synced_weight_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/distributed/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcommunication_op\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mparallel_state\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/distributed/communication_op.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mparallel_state\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tp_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/distributed/parallel_state.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msupports_custom_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcurrent_platform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     direct_register_custom_op(\n\u001b[1;32m    152\u001b[0m         \u001b[0mop_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"all_reduce\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/platforms/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_current_platform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mplatform_cls_qualname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve_current_platform_cls_qualname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             _current_platform = resolve_obj_by_qualname(\n\u001b[0m\u001b[1;32m    277\u001b[0m                 platform_cls_qualname)()\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0m_init_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36mresolve_obj_by_qualname\u001b[0;34m(qualname)\u001b[0m\n\u001b[1;32m   2237\u001b[0m     \"\"\"\n\u001b[1;32m   2238\u001b[0m     \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqualname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2239\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2240\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/platforms/cuda.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# import custom ops, trigger op registration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.11/dist-packages/vllm/_C.abi3.so: undefined symbol: _ZN3c106ivalue14ConstantString6createENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE"
               ]
            }
         ],
         "source": [
            "from vllm import LLM, SamplingParams\n",
            "from transformers import AutoTokenizer\n",
            "\n",
            "model_id = \"speakleash/Bielik-4.5B-v3.0-Instruct-FP8-Dynamic\"\n",
            "\n",
            "sampling_params = SamplingParams(temperature=0.2, top_p=0.95, max_tokens=4096)\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
            "\n",
            "messages = [\n",
            "    {\n",
            "        \"role\": \"system\",\n",
            "        \"content\": \"Jesteś tłumaczem z języka angielskiego na język polski. Użytkownik poda ci tekst po angielsku a ty masz odpowiedzieć pretłumaczonym tekstem. Tłumaczenie ma uwzględniać ton. Ton zostanie podany przez użytkownika po słowie kluczowym TON.\",\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": \"TEXT: 'I would be most grateful if you could kindly inform me at your earliest convenience regarding the status of my application.' TON: nieformalny\",\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"Będę wdzięczny, jeśli dasz mi znać, co z moją aplikacją, jak tylko będziesz mógł.\",\n",
            "    },\n",
            "    {\"role\": \"user\", \"content\": f\"TEXT: '{formal_letter}' TON: skrajnie nieformalny\"},\n",
            "]\n",
            "\n",
            "prompts = tokenizer.apply_chat_template(messages, tokenize=False)\n",
            "\n",
            "llm = LLM(model=model_id, max_model_len=4096)\n",
            "\n",
            "outputs = llm.generate(prompts, sampling_params)\n",
            "\n",
            "generated_text = outputs[0].outputs[0].text\n",
            "print(generated_text)\n",
            "\n",
            "# Can't use 4.5B FP8 Dynamic due to the error: RuntimeError: ('Quantization scheme is not supported for ', 'the current GPU. Min capability: 80. ', 'Current capability: 75.')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "64e22d03-9b30-4015-8286-15b1ee28f05e",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T16:23:02.184199Z",
               "iopub.status.busy": "2025-07-04T16:23:02.183900Z",
               "iopub.status.idle": "2025-07-04T16:23:02.408526Z",
               "shell.execute_reply": "2025-07-04T16:23:02.407464Z",
               "shell.execute_reply.started": "2025-07-04T16:23:02.184179Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "import torch\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "\n",
            "device = \"cuda\"  # the device to load the model onto\n",
            "\n",
            "model_name = \"speakleash/Bielik-4.5B-v3-Instruct\"\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\n",
            "    model_name,\n",
            ")\n",
            "model = AutoModelForCausalLM.from_pretrained(\n",
            "    model_name,\n",
            "    torch_dtype=torch.bfloat16,\n",
            ")\n",
            "\n",
            "messages = [\n",
            "    {\n",
            "        \"role\": \"system\",\n",
            "        \"content\": \"Odpowiadaj krótko, precyzyjnie i wyłącznie w języku polskim.\",\n",
            "    },\n",
            "    {\"role\": \"user\", \"content\": \"Jakie mamy pory roku w Polsce?\"},\n",
            "    {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"W Polsce mamy 4 pory roku: wiosna, lato, jesień i zima.\",\n",
            "    },\n",
            "    {\"role\": \"user\", \"content\": \"Która jest najcieplejsza?\"},\n",
            "]\n",
            "\n",
            "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
            "\n",
            "model_inputs = input_ids.to(device)\n",
            "model.to(device)\n",
            "\n",
            "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
            "decoded = tokenizer.batch_decode(generated_ids)\n",
            "print(decoded[0])"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "3bf1d9b4-c7f7-4c23-b5de-fb253a5115bd",
         "metadata": {},
         "source": [
            "I tried to install few bielik versions bu none of them works. There is a dependency conflict for vllm cuda version and torch cuda version. What's more huggingface access token seem to be really unreliable - sometimes it enables downloading Bielik model and sometimes authentication fails.\n",
            "With that I will just jump to next task.\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "5739577c",
         "metadata": {},
         "source": [
            "## Statistic translation with SMT and performence comparison"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "185f7573-4f1f-40c3-b053-793d8434f378",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T17:02:49.384609Z",
               "iopub.status.busy": "2025-07-04T17:02:49.383841Z",
               "iopub.status.idle": "2025-07-04T17:02:49.390062Z",
               "shell.execute_reply": "2025-07-04T17:02:49.389546Z",
               "shell.execute_reply.started": "2025-07-04T17:02:49.384585Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "import nltk\n",
            "from nltk.tokenize import word_tokenize\n",
            "from nltk.translate import AlignedSent, IBMModel1\n",
            "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
            "from transformers import pipeline\n",
            "\n",
            "try:\n",
            "    nltk.data.find(\"tokenizers/punkt\")\n",
            "except nltk.downloader.DownloadError:\n",
            "    nltk.download(\"punkt\")\n",
            "\n",
            "\n",
            "corpus_en = [\n",
            "    \"i like mac operating system very much\",\n",
            "    \"cs2 is my favourite game\",\n",
            "    \"i love this dog, this is my wife\",\n",
            "]\n",
            "corpus_pl = [\n",
            "    \"bardzo lubię system operacyjny mac\",\n",
            "    \"cs2 to moja ulubiona gra\",\n",
            "    \"kocham tego psa, a to jest moja żona\",\n",
            "]\n",
            "\n",
            "\n",
            "aligned_corpus = [\n",
            "    AlignedSent(word_tokenize(pl), word_tokenize(en))\n",
            "    for en, pl in zip(corpus_en, corpus_pl)\n",
            "]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 91,
         "id": "c4296467-c2d4-4abd-8f78-421ab23e8004",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T17:02:50.205944Z",
               "iopub.status.busy": "2025-07-04T17:02:50.205236Z",
               "iopub.status.idle": "2025-07-04T17:02:50.209215Z",
               "shell.execute_reply": "2025-07-04T17:02:50.208661Z",
               "shell.execute_reply.started": "2025-07-04T17:02:50.205923Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "<AlignedSent: 'bardzo lubię system ...' -> 'i like mac operating...'>\n",
                  "<AlignedSent: 'cs2 to moja ulubiona...' -> 'cs2 is my favourite ...'>\n",
                  "<AlignedSent: 'kocham tego psa , a ...' -> 'i love this dog , th...'>\n"
               ]
            }
         ],
         "source": [
            "for aligned in aligned_corpus:\n",
            "    print(aligned)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 92,
         "id": "6ef9c4bc-cb73-45e3-b0c3-2daa7e67eef6",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T17:02:50.617097Z",
               "iopub.status.busy": "2025-07-04T17:02:50.616883Z",
               "iopub.status.idle": "2025-07-04T17:02:50.625137Z",
               "shell.execute_reply": "2025-07-04T17:02:50.624642Z",
               "shell.execute_reply.started": "2025-07-04T17:02:50.617080Z"
            },
            "trusted": true
         },
         "outputs": [],
         "source": [
            "ibm1 = IBMModel1(aligned_corpus, 20)\n",
            "translation_table = ibm1.translation_table\n",
            "\n",
            "test_sentence_en = \"i like mac operating system very much\"\n",
            "tokenized_test = word_tokenize(test_sentence_en.lower())\n",
            "\n",
            "smt_translation_tokens = []\n",
            "for token in tokenized_test:\n",
            "    if token in translation_table:\n",
            "        best_match = max(\n",
            "            translation_table[token].keys(), key=lambda k: translation_table[token][k]\n",
            "        )\n",
            "        smt_translation_tokens.append(best_match)\n",
            "    else:\n",
            "        smt_translation_tokens.append(token)\n",
            "\n",
            "smt_translation = \" \".join(smt_translation_tokens)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 93,
         "id": "fc8475de-a413-4aff-b47a-e27b3f9db58b",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T17:02:51.095581Z",
               "iopub.status.busy": "2025-07-04T17:02:51.095060Z",
               "iopub.status.idle": "2025-07-04T17:02:52.725329Z",
               "shell.execute_reply": "2025-07-04T17:02:52.724801Z",
               "shell.execute_reply.started": "2025-07-04T17:02:51.095559Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Device set to use cuda\n"
               ]
            }
         ],
         "source": [
            "# transformer translator\n",
            "translator_nmt = pipeline(\n",
            "    \"translation\", model=\"Helsinki-NLP/opus-mt-pl-en\", device=device\n",
            ")  # -1 dla CPU\n",
            "nmt_output = translator_nmt(test_sentence_en)\n",
            "nmt_translation = nmt_output[0][\"translation_text\"]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 94,
         "id": "e135581f-a738-41b0-8101-dff709bab5ee",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-07-04T17:02:57.005640Z",
               "iopub.status.busy": "2025-07-04T17:02:57.004958Z",
               "iopub.status.idle": "2025-07-04T17:02:57.011192Z",
               "shell.execute_reply": "2025-07-04T17:02:57.010567Z",
               "shell.execute_reply.started": "2025-07-04T17:02:57.005614Z"
            },
            "trusted": true
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Zdanie źródłowe:           'i like mac operating system very much'\n",
                  "Tłumaczenie referencyjne:    'bardzo lubię system operacyjny mac'\n",
                  "\n",
                  "Statictical translation\n",
                  "Wynik: 'i like like operating like very much'\n",
                  "Jakość (BLEU): 0.0000\n",
                  "\n",
                  "Transformer translation\n",
                  "Wynik: 'i like mac operating system very much'\n",
                  "Jakość (BLEU): 0.0690\n"
               ]
            }
         ],
         "source": [
            "reference_translation = \"bardzo lubię system operacyjny mac\"\n",
            "reference_tokens = [word_tokenize(reference_translation.lower())]\n",
            "\n",
            "smoothing = SmoothingFunction().method1\n",
            "bleu_smt = sentence_bleu(\n",
            "    reference_tokens,\n",
            "    smt_translation_tokens,\n",
            "    weights=(0.5, 0.5),\n",
            "    smoothing_function=smoothing,\n",
            ")\n",
            "bleu_nmt = sentence_bleu(\n",
            "    reference_tokens,\n",
            "    word_tokenize(nmt_translation.lower()),\n",
            "    weights=(0.5, 0.5),\n",
            "    smoothing_function=smoothing,\n",
            ")\n",
            "\n",
            "print(f\"Zdanie źródłowe:           '{test_sentence_en}'\")\n",
            "print(f\"Tłumaczenie referencyjne:    '{reference_translation}'\\n\")\n",
            "\n",
            "print(\"Statictical translation\")\n",
            "print(f\"Wynik: '{smt_translation}'\")\n",
            "print(f\"Jakość (BLEU): {bleu_smt:.4f}\\n\")\n",
            "\n",
            "print(\"Transformer translation\")\n",
            "print(f\"Wynik: '{nmt_translation}'\")\n",
            "print(f\"Jakość (BLEU): {bleu_nmt:.4f}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "65ee25ef-c26d-43f6-8748-c5d0ed5dcceb",
         "metadata": {},
         "source": [
            "### Final thoughts\n",
            "During the exercises I found out that quality of translation is usually quality/speed/volume tradeoff. Maybe most primitive translation method is SMT (Statictical Machine Translation) which works really fast but does not catch semantics and is not that flexible.\\\n",
            "Translating using transformers really depends on the transformer that was used, how it was trained and dor which languages it is used. As instance mBART model can handle multiple languages however it's heavy and really slow. It also can handle only 1024 tokens at once. Specific transormers designed to deal with 2 languages like _\"Helsinki-NLP/opus-mt-pl-en\"_ have reacher vocabluaries (not generally but for those 2 languages) and can handle semantics relations while being relatively light for the hardware.\\\n",
            "Usage of LLMs like Bielik is not ideal, especially if those are installed locally and not used via API. Even though those can handle many tokens they can be veeery heavy, often need additional configuration and require many dependencies.\\\n",
            "My personal favourite is solution from google. Googletrans lib is fairly small, text does not require any tokenization and cleaning, its fast and covers lot's of languages.\\\\\n",
            "\n",
            "The quality of translation often can't be easly measured and requires human evaluation."
         ]
      }
   ],
   "metadata": {
      "kaggle": {
         "accelerator": "gpu",
         "dataSources": [
            {
               "datasetId": 7800771,
               "sourceId": 12371918,
               "sourceType": "datasetVersion"
            },
            {
               "datasetId": 7800999,
               "sourceId": 12372242,
               "sourceType": "datasetVersion"
            }
         ],
         "dockerImageVersionId": 31041,
         "isGpuEnabled": true,
         "isInternetEnabled": true,
         "language": "python",
         "sourceType": "notebook"
      },
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.10"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
