{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 1: Reprezentacja tekstu przy użyciu metody Bag-of-Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\mstol\\onedrive\\dokumenty\\projects\\wsei-nlp\\.venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\mstol\\onedrive\\dokumenty\\projects\\wsei-nlp\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\mstol\\onedrive\\dokumenty\\projects\\wsei-nlp\\.venv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: gensim in c:\\users\\mstol\\onedrive\\dokumenty\\projects\\wsei-nlp\\.venv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\mstol\\onedrive\\dokumenty\\projects\\wsei-nlp\\.venv\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mstol\\onedrive\\dokumenty\\projects\\wsei-nlp\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mstol\\onedrive\\dokumenty\\projects\\wsei-nlp\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\mstol\\onedrive\\dokumenty\\projects\\wsei-nlp\\.venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mstol\\onedrive\\dokumenty\\projects\\wsei-nlp\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mstol\\onedrive\\dokumenty\\projects\\wsei-nlp\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\mstol\\onedrive\\dokumenty\\projects\\wsei-nlp\\.venv\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\mstol\\onedrive\\dokumenty\\projects\\wsei-nlp\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\mstol\\onedrive\\dokumenty\\projects\\wsei-nlp\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn nltk numpy gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "corpus = fetch_20newsgroups(subset=\"test\")[\"data\"][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [doc.replace(\"\\n\", \"\") for doc in corpus]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('edu', 1502),\n",
       " ('subject', 989),\n",
       " ('com', 949),\n",
       " ('writes', 741),\n",
       " ('article', 612)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_counts = X.toarray().sum(axis=0)\n",
    "\n",
    "vocab_counts = sorted(\n",
    "    zip(vectorizer.get_feature_names_out(), word_counts),\n",
    "    key=lambda item: item[1],\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "vocab_counts = list(\n",
    "    map(lambda row: (row[0], int(row[1])), vocab_counts)\n",
    ")  # numpy init to int\n",
    "\n",
    "display(vocab_counts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2: Obliczanie TF-IDF dla zbioru dokumentów\n",
    "\n",
    "**Polecenie:**\n",
    "- Wczytaj dokumenty z wbudowanego datasetu.\n",
    "- Przetwórz tekst, wykonując tokenizację oraz usuwanie stop-słów.\n",
    "- Oblicz macierz TF-IDF dla całego zbioru dokumentów.\n",
    "- Dla wybranego dokumentu wypisz słowa o najwyższych wartościach TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "corpus = fetch_20newsgroups(subset=\"train\")[\"data\"][:200]\n",
    "# remove eol\n",
    "corpus = [c.replace(\"\\n\", \" \") for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_map = sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '00'), (1, '000'), (2, '0005895485'), (3, '002118'), (4, '002222')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_word_map = {v: k for k, v in word_index_map}\n",
    "display(list(index_word_map.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For doc 0 keyword is 'car'\n",
      "For doc 1 keyword is 'clock'\n",
      "For doc 2 keyword is '180'\n",
      "For doc 3 keyword is 'harris'\n",
      "For doc 4 keyword is 'errors'\n",
      "For doc 5 keyword is 'weapons'\n",
      "For doc 6 keyword is 'bmdelane'\n",
      "For doc 7 keyword is 'scsi'\n",
      "For doc 8 keyword is 'icons'\n",
      "For doc 9 keyword is 'board'\n",
      "For doc 10 keyword is 'irwin'\n",
      "For doc 11 keyword is 'parent'\n",
      "For doc 12 keyword is 'cerkoney'\n",
      "For doc 13 keyword is 'ssf'\n",
      "For doc 14 keyword is 'purchased'\n",
      "For doc 15 keyword is 'mathew'\n",
      "For doc 16 keyword is 'tiff'\n",
      "For doc 17 keyword is 'insurance'\n",
      "For doc 18 keyword is 'amplifier'\n",
      "For doc 19 keyword is 'ncd'\n",
      "For doc 20 keyword is 'keith'\n",
      "For doc 21 keyword is 'captain'\n",
      "For doc 22 keyword is 'catalog'\n",
      "For doc 23 keyword is 'font'\n",
      "For doc 24 keyword is 'scsi'\n",
      "For doc 25 keyword is 'virginia'\n",
      "For doc 26 keyword is 'plants'\n",
      "For doc 27 keyword is 'arizona'\n",
      "For doc 28 keyword is 'god'\n",
      "For doc 29 keyword is 'centerline'\n",
      "For doc 30 keyword is 'stereo'\n",
      "For doc 31 keyword is 'acne'\n",
      "For doc 32 keyword is 'exploding'\n",
      "For doc 33 keyword is 'war'\n",
      "For doc 34 keyword is 'kaldis'\n",
      "For doc 35 keyword is 'mask'\n",
      "For doc 36 keyword is 'moa'\n",
      "For doc 37 keyword is 'encryption'\n",
      "For doc 38 keyword is 'insurance'\n",
      "For doc 39 keyword is 'revolver'\n",
      "For doc 40 keyword is 'bonilla'\n",
      "For doc 41 keyword is 'ncube'\n",
      "For doc 42 keyword is 'western'\n",
      "For doc 43 keyword is 'bullpen'\n",
      "For doc 44 keyword is 'starters'\n",
      "For doc 45 keyword is 'andrew'\n",
      "For doc 46 keyword is 'emulator'\n",
      "For doc 47 keyword is 'movies'\n",
      "For doc 48 keyword is 'toronto'\n",
      "For doc 49 keyword is 'space'\n",
      "For doc 50 keyword is 'failed'\n",
      "For doc 51 keyword is 'sabbath'\n",
      "For doc 52 keyword is 'bj'\n",
      "For doc 53 keyword is 'disease'\n",
      "For doc 54 keyword is 'armenians'\n",
      "For doc 55 keyword is 'stratus'\n",
      "For doc 56 keyword is 'gear'\n",
      "For doc 57 keyword is 'cunyvm'\n",
      "For doc 58 keyword is 'ini'\n",
      "For doc 59 keyword is 'probe'\n",
      "For doc 60 keyword is 'blakey'\n",
      "For doc 61 keyword is 'msi'\n",
      "For doc 62 keyword is 'informix'\n",
      "For doc 63 keyword is 'cds'\n",
      "For doc 64 keyword is 'ford'\n",
      "For doc 65 keyword is 'infinite'\n",
      "For doc 66 keyword is 'tis'\n",
      "For doc 67 keyword is 'atf'\n",
      "For doc 68 keyword is 'religion'\n",
      "For doc 69 keyword is 'suresh'\n",
      "For doc 70 keyword is 'armenian'\n",
      "For doc 71 keyword is 'callison'\n",
      "For doc 72 keyword is 'battery'\n",
      "For doc 73 keyword is 'rdb1'\n",
      "For doc 74 keyword is 'bk'\n",
      "For doc 75 keyword is 'ssc'\n",
      "For doc 76 keyword is 'key'\n",
      "For doc 77 keyword is 'acura'\n",
      "For doc 78 keyword is 'bruno'\n",
      "For doc 79 keyword is 'branham'\n",
      "For doc 80 keyword is 'rushdie'\n",
      "For doc 81 keyword is 'cameras'\n",
      "For doc 82 keyword is 'conference'\n",
      "For doc 83 keyword is 'portal'\n",
      "For doc 84 keyword is 'mr2'\n",
      "For doc 85 keyword is 'olson'\n",
      "For doc 86 keyword is 'mfc'\n",
      "For doc 87 keyword is 'gif'\n",
      "For doc 88 keyword is 'buffalo'\n",
      "For doc 89 keyword is 'spock'\n",
      "For doc 90 keyword is '2600'\n",
      "For doc 91 keyword is 'nazi'\n",
      "For doc 92 keyword is 'rubin'\n",
      "For doc 93 keyword is 'oracle'\n",
      "For doc 94 keyword is 'cray'\n",
      "For doc 95 keyword is 'colorado'\n",
      "For doc 96 keyword is 'reserve'\n",
      "For doc 97 keyword is 'greed'\n",
      "For doc 98 keyword is 'begat'\n",
      "For doc 99 keyword is 'hijaak'\n",
      "For doc 100 keyword is 'windows'\n",
      "For doc 101 keyword is 'ericsson'\n",
      "For doc 102 keyword is 'harleys'\n",
      "For doc 103 keyword is 'cbs'\n",
      "For doc 104 keyword is 'omran'\n",
      "For doc 105 keyword is 'scope'\n",
      "For doc 106 keyword is 'decoupling'\n",
      "For doc 107 keyword is 'balanced'\n",
      "For doc 108 keyword is 'genoa'\n",
      "For doc 109 keyword is 'lorne'\n",
      "For doc 110 keyword is 'cramer'\n",
      "For doc 111 keyword is 'serial'\n",
      "For doc 112 keyword is 'yale'\n",
      "For doc 113 keyword is 'maynard'\n",
      "For doc 114 keyword is 'uiuc'\n",
      "For doc 115 keyword is 'jaeger'\n",
      "For doc 116 keyword is 'dyer'\n",
      "For doc 117 keyword is 'intercon'\n",
      "For doc 118 keyword is 'window'\n",
      "For doc 119 keyword is 'doink'\n",
      "For doc 120 keyword is 'christianity'\n",
      "For doc 121 keyword is 'karr'\n",
      "For doc 122 keyword is 'simms'\n",
      "For doc 123 keyword is 'substitution'\n",
      "For doc 124 keyword is 'hulman'\n",
      "For doc 125 keyword is '42'\n",
      "For doc 126 keyword is 'strom'\n",
      "For doc 127 keyword is 'threat'\n",
      "For doc 128 keyword is 'bradley'\n",
      "For doc 129 keyword is 'hydro'\n",
      "For doc 130 keyword is 'amoco'\n",
      "For doc 131 keyword is 'washington'\n",
      "For doc 132 keyword is 'bach'\n",
      "For doc 133 keyword is 'armenian'\n",
      "For doc 134 keyword is 'launcher'\n",
      "For doc 135 keyword is 'god'\n",
      "For doc 136 keyword is 'stratus'\n",
      "For doc 137 keyword is 'seth'\n",
      "For doc 138 keyword is 'mu'\n",
      "For doc 139 keyword is 'verses'\n",
      "For doc 140 keyword is 'anello'\n",
      "For doc 141 keyword is '333'\n",
      "For doc 142 keyword is 'drivers'\n",
      "For doc 143 keyword is 'encore'\n",
      "For doc 144 keyword is 'ax'\n",
      "For doc 145 keyword is 'quack'\n",
      "For doc 146 keyword is 'video'\n",
      "For doc 147 keyword is 'peace'\n",
      "For doc 148 keyword is 'apostle'\n",
      "For doc 149 keyword is 'eng'\n",
      "For doc 150 keyword is 'sdsu'\n",
      "For doc 151 keyword is 'object'\n",
      "For doc 152 keyword is 'land'\n",
      "For doc 153 keyword is 'shuttle'\n",
      "For doc 154 keyword is 'cpu'\n",
      "For doc 155 keyword is 'new'\n",
      "For doc 156 keyword is 'insurance'\n",
      "For doc 157 keyword is 'comes'\n",
      "For doc 158 keyword is 'cview'\n",
      "For doc 159 keyword is 'technology'\n",
      "For doc 160 keyword is 'water'\n",
      "For doc 161 keyword is 'death'\n",
      "For doc 162 keyword is 'tittle'\n",
      "For doc 163 keyword is '__'\n",
      "For doc 164 keyword is 'harkey'\n",
      "For doc 165 keyword is 'boeing'\n",
      "For doc 166 keyword is 'purdue'\n",
      "For doc 167 keyword is 'ranger'\n",
      "For doc 168 keyword is 'package'\n",
      "For doc 169 keyword is '3mwieu4'\n",
      "For doc 170 keyword is 'cramer'\n",
      "For doc 171 keyword is 'linux'\n",
      "For doc 172 keyword is 'darling'\n",
      "For doc 173 keyword is 'rpi'\n",
      "For doc 174 keyword is 'todd'\n",
      "For doc 175 keyword is 'xterm'\n",
      "For doc 176 keyword is 'ramirez'\n",
      "For doc 177 keyword is 'legal'\n",
      "For doc 178 keyword is 'ibm'\n",
      "For doc 179 keyword is 'braves'\n",
      "For doc 180 keyword is 'tinnitus'\n",
      "For doc 181 keyword is 'cars'\n",
      "For doc 182 keyword is 'dyer'\n",
      "For doc 183 keyword is 'traction'\n",
      "For doc 184 keyword is 'tower'\n",
      "For doc 185 keyword is 'ram'\n",
      "For doc 186 keyword is 'advtech'\n",
      "For doc 187 keyword is 'movie'\n",
      "For doc 188 keyword is 'warren'\n",
      "For doc 189 keyword is 'modem'\n",
      "For doc 190 keyword is 'oracle'\n",
      "For doc 191 keyword is 'donoghue'\n",
      "For doc 192 keyword is 'keys'\n",
      "For doc 193 keyword is 'berkeley'\n",
      "For doc 194 keyword is 'tufts'\n",
      "For doc 195 keyword is 'xv'\n",
      "For doc 196 keyword is 'audio'\n",
      "For doc 197 keyword is 'depression'\n",
      "For doc 198 keyword is 'weapons'\n",
      "For doc 199 keyword is 'hughes'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for idx, doc_vec in enumerate(X.toarray()):\n",
    "    max_val = doc_vec.max()\n",
    "    top_index = np.where(doc_vec == max_val, True, False)\n",
    "    keyword = None\n",
    "    for word, is_top in zip(index_word_map.values(), top_index):\n",
    "        if is_top:\n",
    "            keyword = word\n",
    "            break\n",
    "\n",
    "    print(f\"For doc {idx} keyword is '{keyword}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 3: Implementacja Word Embeddings\n",
    "\n",
    "**Polecenie:**\n",
    "- Wczytaj i przetwórz dokumenty z wbudowanego datasetu (tokenizacja, usuwanie stop-słów, normalizacja).\n",
    "- Wytrenuj model Word Embeddings (np. Word2Vec lub GloVe) na przetworzonym korpusie.\n",
    "- Przetestuj model, wyszukując najbliższe wektory (sąsiadów) dla wybranego słowa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "corpus = fetch_20newsgroups(subset=\"all\", categories=[\"sci.med\"])[\"data\"][:200]\n",
    "corpus = [simple_preprocess(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Word2Vec model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"cancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.24701   ,  0.0738904 , -1.85606   ,  0.5220999 , -0.4313909 ,\n",
       "       -0.418816  ,  1.6845323 ,  0.1948194 ,  0.9034162 ,  0.33131155,\n",
       "        0.6130391 ,  2.3205917 , -2.249751  ,  1.5947751 ,  1.8228657 ,\n",
       "        6.0251913 ,  2.107819  ,  3.5900204 ,  0.52047676,  0.61176765,\n",
       "       -1.2389067 ,  1.7382876 , -1.576263  , -0.5855002 ,  1.2057762 ,\n",
       "       -0.23583856, -0.57232213,  0.09234209,  0.08178885, -0.0308151 ,\n",
       "       -2.2027695 ,  1.2682238 ,  4.0586715 ,  1.2193611 , -1.7035402 ,\n",
       "       -0.1873678 ,  0.9470418 ,  1.6737384 , -2.0647795 , -1.476369  ,\n",
       "        2.5199263 , -2.0613866 ,  0.35537577,  1.0557905 ,  0.75583893,\n",
       "        0.40234601, -2.0839672 ,  0.66014606, -0.25841245,  4.7200446 ,\n",
       "       -0.75552624, -1.4668808 , -3.0472383 ,  1.3335482 , -1.1121557 ,\n",
       "       -0.99049634,  1.315247  , -0.30537477,  0.4555336 ,  0.386316  ,\n",
       "        0.6718272 , -0.3943564 , -2.4939244 ,  1.4112414 , -3.1052403 ,\n",
       "        0.47708905, -0.2374872 , -1.2442319 ,  1.9624093 , -0.8402056 ,\n",
       "        0.68960154,  2.8750887 , -0.38417155, -0.89246714,  2.5893779 ,\n",
       "       -0.03320413, -1.7704053 , -2.3349595 , -1.2644737 ,  3.6394653 ,\n",
       "       -0.4735256 ,  1.8117204 , -1.1808535 ,  0.297093  ,  1.1396161 ,\n",
       "       -0.79204255, -1.7320561 ,  1.7782305 ,  1.8689888 ,  1.4501379 ,\n",
       "        0.18002625, -1.2913762 ,  0.34628168, -0.88049865,  2.9654706 ,\n",
       "        0.52830434,  4.566019  , -1.7278565 ,  0.21973638, -0.8522104 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector for the word\n",
    "model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [r[0] for r in model.wv.most_similar(word)]\n",
    "vectors = [model.wv[name] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nicotine', 'cells', 'lung', 'leukemia', 'risk']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 4: Generowanie i analiza Bigramów\n",
    "\n",
    "**Polecenie:**\n",
    "- Wczytaj dokumenty z wbudowanego datasetu.\n",
    "- Wykonaj tokenizację tekstu oraz usuń stop-słowa.\n",
    "- Wygeneruj bigramy (pary kolejnych słów) z przetworzonego tekstu.\n",
    "- Wyświetl najczęściej występujące bigramy wraz z ich liczebnością."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\mstol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "nltk.download(\"reuters\")\n",
    "\n",
    "raw_text = reuters.raw()\n",
    "tokens = simple_preprocess(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\mstol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "nltk.download(\"reuters\")\n",
    "\n",
    "raw_text = reuters.raw()\n",
    "tokens = simple_preprocess(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = list(nltk.bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_count = {b: 0 for b in bigrams}\n",
    "\n",
    "for b in bigrams:\n",
    "    bigrams_count[b] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('in', 'the'), 7093),\n",
       " (('of', 'the'), 6913),\n",
       " (('said', 'the'), 5355),\n",
       " (('mln', 'dlrs'), 4472),\n",
       " (('said', 'it'), 4367)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sorted(bigrams_count.items(), key=lambda item: item[1], reverse=True)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 5: Analiza Trigramów w tekście\n",
    "\n",
    "**Polecenie:**\n",
    "- Wczytaj dokumenty z wbudowanego datasetu.\n",
    "- Przetwórz tekst, wykonując tokenizację oraz usuwanie stop-słów.\n",
    "- Wygeneruj trigramy (sekwencje trzech kolejnych słów) z dokumentów.\n",
    "- Wypisz najczęściej występujące trigramy w analizowanym zbiorze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\mstol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "nltk.download(\"reuters\")\n",
    "\n",
    "raw_text = reuters.raw()\n",
    "tokens = simple_preprocess(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_count = {t: 0 for t in trigrams}\n",
    "\n",
    "for t in trigrams:\n",
    "    trigrams_count[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('mln', 'vs', 'mln'), 3402),\n",
       " (('cts', 'vs', 'cts'), 1779),\n",
       " (('revs', 'mln', 'vs'), 1515),\n",
       " (('shr', 'cts', 'vs'), 1446),\n",
       " (('the', 'company', 'said'), 1181)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sorted(trigrams_count.items(), key=lambda item: item[1], reverse=True)[:5])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
